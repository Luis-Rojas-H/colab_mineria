# ðŸ“š FEEDBACK: ÃRBOLES DE DECISIÃ“N Y RANDOM FOREST

## ðŸ“Œ NOMBRE DEL ARCHIVO

**`09_Decision_Trees_RandomForest_Classification.ipynb`**

**Por quÃ© este nombre:**
- `09` = Tema 9 (Ãrboles de DecisiÃ³n y Ensambles)
- `Decision_Trees_RandomForest` = Algoritmos usados
- `Classification` = Tipo de problema
- `.ipynb` = Formato ejecutable

---

## ðŸŽ¯ Â¿QUÃ‰ ES ESTE NOTEBOOK?

Implementa **3 enfoques distintos** de clasificaciÃ³n:

```
1. Ãrboles de DecisiÃ³n Simples (depth=1)
2. Ãrboles de DecisiÃ³n Completos (sin restricciones)
3. Ãrboles Optimizados (GridSearchCV)
4. Random Forest (Ensamble)
```

---

## ðŸ“‚ ESTRUCTURA DEL NOTEBOOK (14 Secciones)

### **PARTE 1: ÃRBOLES DE DECISIÃ“N**

```
1. Importaciones
   â†“
2. Ãrbol Simple (max_depth=1, Mowers dataset)
   â”œâ”€ Ãrbol muy simple para entender concepto
   â””â”€ FÃ¡cil de visualizar
   
3. Ãrbol Completo (sin restricciones, UniversalBank)
   â”œâ”€ Ãrbol sin lÃ­mites
   â””â”€ Propenso a overfitting
   
4. EvaluaciÃ³n del Ã¡rbol completo
   â”œâ”€ Compara train vs validation
   â””â”€ Observa overfitting
   
5. Cross-Validation (5-fold)
   â””â”€ ValidaciÃ³n mÃ¡s robusta
   
6. Ãrbol Podado (Pruned tree)
   â”œâ”€ Con restricciones (max_depth, min_samples_split)
   â””â”€ MÃ¡s pequeÃ±o y generalizable
   
7. EvaluaciÃ³n del Ã¡rbol podado
   â””â”€ Mejor balance train/validation
   
8. GridSearchCV - BÃºsqueda inicial
   â”œâ”€ BÃºsqueda gruesa de parÃ¡metros
   â””â”€ Identifica zona promisoria
   
9. GridSearchCV - BÃºsqueda refinada
   â”œâ”€ BÃºsqueda fina alrededor de buenos parÃ¡metros
   â””â”€ Encuentra Ã³ptimos locales
   
10. EvaluaciÃ³n del Ã¡rbol optimizado
    â””â”€ El mejor Ã¡rbol que encontramos
```

### **PARTE 2: RANDOM FOREST**

```
11. Random Forest Classifier
    â”œâ”€ 500 Ã¡rboles
    â”œâ”€ Cada uno entrenado con muestra aleatoria
    â””â”€ PredicciÃ³n = promedio de todos
    
12. Feature Importance
    â”œâ”€ QuÃ© variables son mÃ¡s importantes
    â”œâ”€ GrÃ¡fico de barras
    â””â”€ Barras de error (desviaciÃ³n estÃ¡ndar)
    
13. EvaluaciÃ³n Random Forest
    â””â”€ Accuracy en validation
    
14. Resumen Comparativo
    â””â”€ ComparaciÃ³n de los 3 modelos
```

---

## ðŸ”‘ CONCEPTOS CLAVE

### **Ãrbol de DecisiÃ³n (Decision Tree)**

```
Â¿QuÃ© es?
â”œâ”€ Modelo que divide datos en regiones
â”œâ”€ Usa divisiÃ³n binaria (if/else)
â””â”€ Crea reglas legibles

Ventajas:
â”œâ”€ FÃ¡cil de entender y explicar
â”œâ”€ No requiere normalizaciÃ³n
â””â”€ Maneja categorÃ­as bien

Desventajas:
â”œâ”€ Propenso a overfitting
â”œâ”€ Inestable (pequeÃ±os cambios = Ã¡rbol diferente)
â””â”€ Sesgado a variables de alta cardinalidad

ParÃ¡metros importantes:
â”œâ”€ max_depth: Profundidad mÃ¡xima (controla tamaÃ±o)
â”œâ”€ min_samples_split: MÃ­nimo para dividir (controla crecimiento)
â””â”€ min_impurity_decrease: Mejora mÃ­nima (previene divisiones dÃ©biles)
```

### **Random Forest (Ensamble)**

```
Â¿QuÃ© es?
â”œâ”€ Conjunto de Ã¡rboles de decisiÃ³n
â”œâ”€ Cada Ã¡rbol se entrena con bootstrap (muestra con reemplazo)
â”œâ”€ Cada Ã¡rbol usa caracterÃ­sticas aleatorias
â””â”€ PredicciÃ³n = promedio de todos los Ã¡rboles

Ventajas:
â”œâ”€ Reduce overfitting significativamente
â”œâ”€ MÃ¡s robusto que Ã¡rbol individual
â”œâ”€ Mejor generalizaciÃ³n
â”œâ”€ Proporciona Feature Importance

Desventajas:
â”œâ”€ Menos interpretable (no ves el Ã¡rbol final)
â”œâ”€ MÃ¡s lento de entrenar (pero predice rÃ¡pido)
â””â”€ Usa mÃ¡s memoria

ParÃ¡metro clave:
â””â”€ n_estimators: NÃºmero de Ã¡rboles (tÃ­picamente 100-1000)
```

### **GridSearchCV**

```
Â¿QuÃ© es?
â”œâ”€ BÃºsqueda exhaustiva de hiperparÃ¡metros
â”œâ”€ Prueba todas las combinaciones posibles
â””â”€ Usa cross-validation para evaluar

Proceso:
1. Define grid de parÃ¡metros a probar
2. Para cada combinaciÃ³n:
   â”œâ”€ Entrena modelo
   â”œâ”€ EvalÃºa con cross-validation
   â””â”€ Guarda score
3. Retorna mejor combinaciÃ³n

Ejemplo:
â”œâ”€ BÃºsqueda 1 (gruesa): 100 combinaciones
â”œâ”€ Analiza resultados
â””â”€ BÃºsqueda 2 (fina): 500 combinaciones en zona promisoria
```

---

## ðŸ“Š Â¿QUÃ‰ DEBES ANALIZAR?

### **SecciÃ³n 2-4: Ãrbol Simple y Completo**

```
Pregunta: Â¿Por quÃ© el Ã¡rbol completo tiene accuracy=1.0 en training?
Respuesta: Overfitting - memorizÃ³ el dataset de training

Pregunta: Â¿Por quÃ© baja el accuracy en validation?
Respuesta: No generalizÃ³ bien - no vio esos casos antes
```

### **SecciÃ³n 5: Cross-Validation**

```
Pregunta: Â¿Son similares los 5 folds?
Respuesta: Si son parecidos â†’ modelo estable
           Si varÃ­an mucho â†’ modelo inestable

Pregunta: Â¿Es mejor el Ã¡rbol completo que el promedio de CV?
Respuesta: No necesariamente - CV es mÃ¡s confiable
```

### **SecciÃ³n 6-7: Ãrbol Podado**

```
Pregunta: Â¿Mejora vs Ã¡rbol completo?
Respuesta: Training: baja (es mÃ¡s simple)
           Validation: sube (generaliza mejor)
           
Pregunta: Â¿CuÃ¡l es el balance ideal?
Respuesta: Cuando train â‰ˆ validation (sin gran diferencia)
```

### **SecciÃ³n 8-10: GridSearchCV y OptimizaciÃ³n**

```
Pregunta: Â¿CuÃ¡ntas combinaciones se prueban?
BÃºsqueda 1: 4 Ã— 5 Ã— 5 = 100 combinaciones
BÃºsqueda 2: 14 Ã— 12 Ã— 3 = 504 combinaciones

Pregunta: Â¿Por quÃ© dos bÃºsquedas?
Respuesta: 1) ExploraciÃ³n gruesa â†’ zona promisoria
           2) ExploraciÃ³n fina â†’ Ã³ptimo local
```

### **SecciÃ³n 12: Feature Importance**

```
Pregunta: Â¿QuÃ© significan las barras?
Respuesta: Altura = importancia
           LÃ­nea de error = variabilidad entre Ã¡rboles

Pregunta: Â¿CuÃ¡les son las variables mÃ¡s importantes?
Respuesta: Las mÃ¡s largas (mayores importancias)

Pregunta: Â¿Por quÃ© Random Forest puede decir importancia?
Respuesta: Cada Ã¡rbol contribuye diferente
           Promediamos importancias = variable importance del forest
```

---

## ðŸŽ¯ PARA TU EXAMEN

### **Pregunta 1: Â¿Diferencia entre Decision Tree y Random Forest?**

```
Decision Tree:
â”œâ”€ Un solo Ã¡rbol
â”œâ”€ Interpre table
â””â”€ Propenso a overfitting

Random Forest:
â”œâ”€ Muchos Ã¡rboles (tÃ­picamente 500)
â”œâ”€ Menos interpretable pero mÃ¡s robusto
â””â”€ Mejor generalizaciÃ³n
```

### **Pregunta 2: Â¿Por quÃ© GridSearchCV?**

```
Sin GridSearchCV:
â””â”€ Adivinas parÃ¡metros (trial and error)

Con GridSearchCV:
â”œâ”€ Prueba combinaciones sistemÃ¡ticamente
â”œâ”€ Usa cross-validation (mÃ¡s confiable)
â””â”€ Encuentra buenos parÃ¡metros automÃ¡ticamente
```

### **Pregunta 3: Â¿CÃ³mo sÃ© si hay overfitting?**

```
Indicadores:
â”œâ”€ train_accuracy >> valid_accuracy
â”œâ”€ train error muy bajo, valid error alto
â””â”€ Ãrbol muy profundo

Soluciones:
â”œâ”€ Reducir max_depth
â”œâ”€ Aumentar min_samples_split
â”œâ”€ Usar Random Forest en lugar de Ã¡rbol simple
â””â”€ MÃ¡s datos (si es posible)
```

### **Pregunta 4: Â¿QuÃ© es Feature Importance?**

```
Concepto:
â”œâ”€ Mide cuÃ¡n importante es cada variable
â”œâ”€ Se calcula cÃ³mo contribuye a reducir impureza
â””â”€ Valores entre 0 y 1 (suma a 1.0)

InterpretaciÃ³n:
â”œâ”€ Importancia > 0.2 = muy importante
â”œâ”€ Importancia 0.05-0.2 = moderadamente importante
â””â”€ Importancia < 0.05 = poco importante
```

---

## ðŸ“ COMPARACIÃ“N CON OTRAS TÃ‰CNICAS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CaracterÃ­stica   â”‚ Decision Tree   â”‚ Random Forest    â”‚ Naive Bayes     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Interpretable    â”‚ âœ… Muy sÃ­       â”‚ âš ï¸ Poco          â”‚ âœ… Muy sÃ­       â”‚
â”‚ Velocidad train  â”‚ âœ… RÃ¡pido       â”‚ âš ï¸ Medio         â”‚ âœ… Muy rÃ¡pido   â”‚
â”‚ Velocidad pred   â”‚ âœ… Muy rÃ¡pido   â”‚ âš ï¸ Lento (500Ã¡rboles)             â”‚
â”‚ Overfitting      â”‚ âŒ Alto riesgo  â”‚ âœ… Muy resistenteâ”‚ âš ï¸ Medio        â”‚
â”‚ Datos continuos  â”‚ âœ… SÃ­           â”‚ âœ… SÃ­            â”‚ âš ï¸ Requiere binsâ”‚
â”‚ Datos discretos  â”‚ âœ… SÃ­           â”‚ âœ… SÃ­            â”‚ âœ… Ideal        â”‚
â”‚ Escalado needed  â”‚ âŒ No           â”‚ âŒ No            â”‚ âš ï¸ Recomendado  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… CHECKLIST PRE-EXAMEN

- [ ] Entiendo la diferencia entre Decision Tree y Random Forest
- [ ] Puedo explicar quÃ© es overfitting
- [ ] Entiendo cÃ³mo funciona GridSearchCV
- [ ] SÃ© quÃ© significan los parÃ¡metros (max_depth, min_samples_split)
- [ ] Puedo interpretar una matriz de confusiÃ³n
- [ ] Entiendo Feature Importance
- [ ] Puedo leer un Ã¡rbol de decisiÃ³n (cuando se plotea)
- [ ] Conozco ventajas y desventajas de cada modelo
- [ ] SÃ© cuÃ¡ndo usar Decision Tree vs Random Forest vs Naive Bayes

---

## ðŸš€ CÃ“MO EJECUTAR

```bash
# Requisitos (si no los tienes aÃºn):
pip install scikit-learn pandas numpy matplotlib seaborn dmba jupyter

# Ejecutar:
cd "c:\Users\USUARIO\Documents\GitHub\colab_mineria"
jupyter notebook 09_Decision_Trees_RandomForest_Classification.ipynb
```

---

## ðŸŽ“ CONCEPTOS CLAVE A RECORDAR

```
1. Decision Tree = Un Ã¡rbol que divide recursivamente
2. Random Forest = MÃºltiples Ã¡rboles para mayor robustez
3. Overfitting = Alta precisiÃ³n en train, baja en validation
4. Underfitting = Baja precisiÃ³n en ambos (modelo muy simple)
5. GridSearchCV = BÃºsqueda automÃ¡tica de mejores hiperparÃ¡metros
6. Feature Importance = QuÃ© variables influyen mÃ¡s en predicciones
7. Cross-Validation = Evaluar modelo sin depender de una divisiÃ³n especÃ­fica
8. Bootstrap = Muestra con reemplazo (base de Random Forest)
```

---

**Â¡Ã‰xito en tu examen!** ðŸ†
