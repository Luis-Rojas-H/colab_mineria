# Gu√≠a de Estudio: An√°lisis de Componentes Principales (PCA)

## Tabla de Contenidos
1. [Introducci√≥n al PCA](#introducci√≥n-al-pca)
2. [Fundamentos Matem√°ticos](#fundamentos-matem√°ticos)
3. [¬øPor qu√© PCA? Motivaci√≥n](#por-qu√©-pca-motivaci√≥n)
4. [Tipo de Datos: Discretos vs Continuos](#tipo-de-datos-discretos-vs-continuos)
5. [An√°lisis del C√≥digo Paso a Paso](#an√°lisis-del-c√≥digo-paso-a-paso)
6. [Selecci√≥n de Componentes Principales](#selecci√≥n-de-componentes-principales)
7. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
8. [Sparse PCA](#sparse-pca)
9. [Ejercicios de Estudio](#ejercicios-de-estudio)

---

## Introducci√≥n al PCA

### ¬øQu√© es PCA?

El **An√°lisis de Componentes Principales (PCA)** es una t√©cnica de reducci√≥n de dimensionalidad que transforma un conjunto de variables posiblemente correlacionadas en un conjunto m√°s peque√±o de variables no correlacionadas llamadas **componentes principales**.

### Objetivos del PCA:
1. **Reducir dimensionalidad**: Pasar de `p` variables a `k` componentes (donde `k < p`)
2. **Eliminar redundancia**: Remover correlaciones entre variables
3. **Visualizaci√≥n**: Facilitar la visualizaci√≥n de datos de alta dimensi√≥n
4. **Reducci√≥n de ruido**: Conservar solo la informaci√≥n m√°s relevante

---

## Fundamentos Matem√°ticos

### 1. Matriz de Datos

Consideremos una matriz de datos **X** de dimensi√≥n `n √ó p`:
- `n` = n√∫mero de observaciones
- `p` = n√∫mero de variables

```
X = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çö]  donde cada x·µ¢ es un vector de n observaciones
```

### 2. Matriz de Covarianza

La matriz de covarianza **Œ£** (o **S** para la muestra) es:

```
S = (1/(n-1)) ¬∑ X·µÄ ¬∑ X  (si X est√° centrada)
```

**Elementos de la matriz:**
- Diagonal: Varianzas de cada variable
- Fuera de diagonal: Covarianzas entre pares de variables

**¬øPor qu√© es importante?**
- Mide c√≥mo las variables var√≠an conjuntamente
- PCA busca las direcciones de m√°xima varianza en esta matriz

### 3. Valores y Vectores Propios

**Problema de eigenvalores:**
```
S ¬∑ v = Œª ¬∑ v
```

Donde:
- **Œª** (lambda) = valor propio (eigenvalue)
- **v** = vector propio (eigenvector)

**En PCA:**
- Los **vectores propios** definen las direcciones de los componentes principales
- Los **valores propios** indican la varianza explicada por cada componente

### 4. Descomposici√≥n Espectral

La matriz de covarianza S se puede descomponer como:

```
S = V ¬∑ Œõ ¬∑ V·µÄ
```

Donde:
- **V** = matriz de vectores propios (columnas)
- **Œõ** = matriz diagonal de valores propios
- **V·µÄ** = transpuesta de V

### 5. Componentes Principales

Las componentes principales son combinaciones lineales de las variables originales:

```
PC‚ÇÅ = v‚ÇÅ‚ÇÅ¬∑x‚ÇÅ + v‚ÇÅ‚ÇÇ¬∑x‚ÇÇ + ... + v‚ÇÅ‚Çö¬∑x‚Çö
PC‚ÇÇ = v‚ÇÇ‚ÇÅ¬∑x‚ÇÅ + v‚ÇÇ‚ÇÇ¬∑x‚ÇÇ + ... + v‚ÇÇ‚Çö¬∑x‚Çö
...
```

**Propiedades importantes:**
1. **PC‚ÇÅ** tiene la m√°xima varianza posible
2. **PC‚ÇÇ** tiene la m√°xima varianza posible ortogonal a PC‚ÇÅ
3. Y as√≠ sucesivamente...
4. Las PCs son **ortogonales** entre s√≠ (no correlacionadas)

### 6. Scores vs Loadings

**Loadings (Cargas):**
- Son los coeficientes `v·µ¢‚±º` en las ecuaciones anteriores
- Son los vectores propios de la matriz de covarianza
- Indican c√≥mo contribuye cada variable original a cada PC

**Scores (Puntuaciones):**
- Son los valores de las observaciones en el nuevo sistema de coordenadas
- Se calculan como: `Scores = X ¬∑ V`
- Son las proyecciones de los datos en las direcciones de las PCs

---

## ¬øPor qu√© PCA? Motivaci√≥n

### Problema: Maldici√≥n de la Dimensionalidad

Cuando tenemos muchas variables (`p` grande):
- Visualizaci√≥n dif√≠cil (solo podemos ver en 2D o 3D)
- Modelos complejos y sobreajuste
- Variables redundantes (correlacionadas)
- Mayor costo computacional

### Soluci√≥n: PCA

**Ejemplo conceptual:**

Imagina que tienes datos sobre cereales con 13 variables:
```
calories, protein, fat, sodium, fiber, carbo, sugars, potass, 
vitamins, shelf, weight, cups, rating
```

Muchas de estas variables est√°n correlacionadas:
- **Calor√≠as** ‚Üî Carbohidratos (alta correlaci√≥n positiva)
- **Fibra** ‚Üî Rating (correlaci√≥n positiva)
- **Az√∫car** ‚Üî Rating (correlaci√≥n negativa)

**PCA encuentra:**
- PC‚ÇÅ podr√≠a representar "contenido energ√©tico general"
- PC‚ÇÇ podr√≠a representar "salud nutricional"
- PC‚ÇÉ podr√≠a representar "contenido de vitaminas"

En lugar de 13 variables, podemos trabajar con 2-3 componentes que capturan el 80-90% de la informaci√≥n.

---

## Tipo de Datos: Discretos vs Continuos

### ¬øPCA funciona con datos discretos o continuos?

**Respuesta corta:** PCA est√° dise√±ado principalmente para **variables continuas**.

### Variables Continuas ‚úÖ

**Por qu√© funciona bien:**
1. Las variables continuas tienen varianza continua
2. Las distancias euclidianas tienen sentido
3. La covarianza es interpretable
4. Ejemplos: altura, peso, temperatura, precio

**En nuestro c√≥digo:**
```python
cereals_df[["calories", "rating"]]
```
- `calories`: continua (50, 100, 110, etc.)
- `rating`: continua (escala de evaluaci√≥n)

### Variables Discretas ‚ö†Ô∏è

**Precauci√≥n con:**
1. **Variables categ√≥ricas nominales** (rojo, azul, verde)
   - NO usar PCA directamente
   - Necesitan codificaci√≥n especial (dummy variables)
   
2. **Variables ordinales** (bajo, medio, alto)
   - Se puede usar si hay suficientes categor√≠as
   - Asumir distancias iguales entre categor√≠as
   
3. **Variables de conteo** (0, 1, 2, 3, ...)
   - Se puede usar si el rango es amplio
   - Considerar transformaciones (log, sqrt)

### ¬øPor qu√© la distinci√≥n es importante?

**Matem√°ticamente:**
- PCA calcula distancias euclidianas: `d = ‚àö((x‚ÇÅ-y‚ÇÅ)¬≤ + (x‚ÇÇ-y‚ÇÇ)¬≤ + ...)`
- Para variables continuas, estas distancias son significativas
- Para categ√≥ricas, "rojo" - "azul" no tiene sentido matem√°tico

**Ejemplo:**
```
Variables continuas:
  Altura: 170 cm vs 175 cm ‚Üí diferencia = 5 cm (interpretable)
  
Variables categ√≥ricas:
  Color: 1 (rojo) vs 2 (azul) ‚Üí diferencia = 1 (sin sentido)
```

### Recomendaciones

**Usa PCA cuando:**
- ‚úÖ Todas las variables son continuas
- ‚úÖ Variables ordinales con muchas categor√≠as (>5)
- ‚úÖ Variables de conteo con rango amplio

**NO uses PCA cuando:**
- ‚ùå Variables categ√≥ricas nominales
- ‚ùå Variables binarias (0/1)
- ‚ùå Mezcla de tipos de variables sin preprocesamiento

**Alternativas:**
- **MCA** (Multiple Correspondence Analysis) para categ√≥ricas
- **FAMD** (Factor Analysis of Mixed Data) para mixtas

---

## An√°lisis del C√≥digo Paso a Paso

### Secci√≥n 1: PCA en Dos Variables (Calor√≠as y Rating)

```python
cereals_df = pd.read_csv("/content/Cereals.csv")
pcs = PCA(n_components=2)
pcs.fit(cereals_df[["calories", "rating"]])
```

**¬øPor qu√© estas dos variables?**

1. **Prop√≥sito did√°ctico**: Con 2 variables podemos visualizar en 2D
2. **Posible correlaci√≥n**: Exploramos si hay relaci√≥n entre calor√≠as y rating
3. **Interpretaci√≥n simple**: F√°cil de entender para aprendizaje

**Matem√°ticamente:**
- X es una matriz `n √ó 2`
- Solo habr√° 2 componentes principales posibles
- min(n, p) = min(n, 2) = 2

### Resumen de PCA (pcsSummary)

```python
pcsSummary = pd.DataFrame({
    "Standard deviation": np.sqrt(pcs.explained_variance_),
    "Proportion of variance": pcs.explained_variance_ratio_,
    "Cumulative proportion": np.cumsum(pcs.explained_variance_ratio_)
})
```

**Interpretaci√≥n de cada m√©trica:**

1. **Standard deviation (Desviaci√≥n est√°ndar)**
   ```
   SD_i = ‚àö(Œª·µ¢)
   ```
   - Es la ra√≠z cuadrada del valor propio i
   - Mide la dispersi√≥n de los datos en la direcci√≥n de PC·µ¢
   - Mayor SD ‚Üí m√°s informaci√≥n capturada

2. **Proportion of variance (Proporci√≥n de varianza)**
   ```
   PropVar_i = Œª·µ¢ / Œ£Œª‚±º
   ```
   - Fracci√≥n de la varianza total explicada por PC·µ¢
   - Valores entre 0 y 1
   - Suma de todas las proporciones = 1

3. **Cumulative proportion (Proporci√≥n acumulada)**
   ```
   CumProp_k = Œ£(Œª·µ¢ / Œ£Œª‚±º)  para i=1 hasta k
   ```
   - Varianza total explicada hasta el componente k
   - √ötil para decidir cu√°ntos componentes retener

**Ejemplo de interpretaci√≥n:**
```
         PC1    PC2
SD       15.2   8.3
PropVar  0.77   0.23
CumProp  0.77   1.00
```
Significa:
- PC1 explica 77% de la varianza total
- PC2 explica 23% adicional
- Juntos explican 100% (porque solo hay 2 variables)

### Components (Loadings/Cargas)

```python
pcsComponents_df = pd.DataFrame(
    pcs.components_.transpose(), 
    columns=["PC1", "PC2"],
    index=["calories", "rating"]
)
```

**¬øQu√© representan?**

Matriz de vectores propios:
```
          PC1     PC2
calories  0.89   -0.45
rating    0.45    0.89
```

**Interpretaci√≥n:**
- **PC1 = 0.89¬∑calories + 0.45¬∑rating**
  - Dominada por calor√≠as (coeficiente mayor)
  - Ambas variables contribuyen positivamente
  - Representa algo como "intensidad nutricional"

- **PC2 = -0.45¬∑calories + 0.89¬∑rating**
  - Dominada por rating
  - Calor√≠as contribuyen negativamente
  - Representa algo como "calidad vs cantidad"

**Propiedades matem√°ticas:**
1. Cada columna tiene norma 1: `‚àö(0.89¬≤ + 0.45¬≤) = 1`
2. Las columnas son ortogonales: `PC1 ¬∑ PC2 = 0`

### Scores (Puntuaciones)

```python
scores = pd.DataFrame(
    pcs.transform(cereals_df[["calories", "rating"]]),
    columns=["PC1","PC2"]
)
```

**¬øQu√© son?**
- Son las coordenadas de cada observaci√≥n (cereal) en el nuevo sistema
- Se calculan como: `Score_i = X·µ¢ ¬∑ V`

**Ejemplo:**
Si un cereal tiene:
- calories = 100
- rating = 50

Su score ser√≠a:
```
PC1_score = 0.89 √ó 100 + 0.45 √ó 50 = 89 + 22.5 = 111.5
PC2_score = -0.45 √ó 100 + 0.89 √ó 50 = -45 + 44.5 = -0.5
```

**Utilidad:**
- Cada fila = un cereal en el espacio de componentes principales
- Podemos plotear estos scores para visualizar patrones
- Detectar outliers
- Hacer clustering en espacio reducido

### Visualizaci√≥n: Scatterplot de Scores

```python
plt.plot(scores.PC1[:], scores.PC2[:], 'o')
```

**¬øQu√© observamos?**
- Cada punto = un cereal
- Posici√≥n en PC1 y PC2
- Patrones, clusters, outliers
- Relaciones no visibles en variables originales

---

### Secci√≥n 2: PCA con Escalamiento en Todas las Variables

```python
pcs = PCA()
pcs.fit(preprocessing.scale(cereals_df.iloc[:, 3:].dropna(axis=0)))
```

**Cambios importantes:**

1. **`PCA()` sin n_components**
   - Calcula TODAS las componentes posibles
   - min(n, p) componentes
   
2. **`preprocessing.scale()`**
   - ¬°MUY IMPORTANTE!
   - Estandariza las variables: media=0, varianza=1
   
3. **`cereals_df.iloc[:, 3:]`**
   - Toma desde la columna 4 en adelante
   - Asume que las primeras 3 son identificadores o categ√≥ricas

### ¬øPor qu√© escalar? üî• CR√çTICO

**Problema sin escalar:**

Imagina variables con diferentes unidades:
```
calories:  [50, 100, 150]     (rango: 100)
sodium:    [0, 200, 400]      (rango: 400)
fiber:     [1, 3, 5]          (rango: 4)
```

**PCA sin escalar:**
- Sodium dominar√≠a (mayor varianza por escala)
- PC1 ‚âà sodium casi completamente
- Informaci√≥n de fiber se perder√≠a

**PCA con escalar:**
```
calories:  [-1.22, 0, 1.22]   (varianza: 1)
sodium:    [-1.22, 0, 1.22]   (varianza: 1)
fiber:     [-1.22, 0, 1.22]   (varianza: 1)
```
- Todas las variables tienen igual "peso"
- Las componentes reflejan correlaciones, no escalas

**Regla:**
- ‚úÖ **SIEMPRE escalar** cuando las variables tienen diferentes unidades
- ‚ö†Ô∏è Puede no escalar si todas las variables est√°n en la misma escala

**Matem√°ticamente:**
```python
x_scaled = (x - mean(x)) / std(x)
```

Esto convierte la matriz de covarianza en matriz de correlaci√≥n:
```
Cov(X_scaled) = Corr(X)
```

### Visualizaci√≥n 3D

```python
ax = plt.axes(projection='3d')
ax.scatter3D(xline, yline, zline, cmap='Greens')
```

**Utilidad:**
- Visualizar datos en las 3 primeras PCs
- Ver estructura de datos en 3D
- Las 3 primeras PCs usualmente capturan 70-90% de varianza

---

### Secci√≥n 3: Sparse PCA

```python
spca = SparsePCA(random_state=0, alpha=1e-3, ridge_alpha=1e-6)
```

**¬øQu√© es Sparse PCA?**

**Problema del PCA est√°ndar:**
- Los loadings (cargas) suelen ser TODOS no-cero
- Dif√≠cil interpretaci√≥n: todas las variables contribuyen un poco

**Ejemplo PCA regular:**
```
PC1 = 0.23¬∑cal + 0.18¬∑prot + 0.31¬∑fat + 0.22¬∑sod + ...
```
Todas las variables aparecen, dif√≠cil de interpretar.

**Sparse PCA:**
- Fuerza muchos loadings a ser EXACTAMENTE cero
- Solo unas pocas variables por componente

**Ejemplo Sparse PCA:**
```
PC1 = 0.50¬∑cal + 0¬∑prot + 0.50¬∑fat + 0¬∑sod + ...
```
Solo calor√≠as y grasa importan, ¬°mucho m√°s interpretable!

**Trade-off:**
- ‚úÖ M√°s interpretable
- ‚úÖ Menos variables por componente
- ‚ùå Pierde un poco de varianza explicada
- ‚ùå M√°s costoso computacionalmente

**Par√°metros:**
- `alpha`: Controla el nivel de sparsity (mayor = m√°s ceros)
- `ridge_alpha`: Regularizaci√≥n adicional para estabilidad

**Matem√°ticamente:**

PCA minimiza:
```
||X - X¬∑V¬∑V·µÄ||¬≤
```

Sparse PCA minimiza:
```
||X - X¬∑V¬∑V·µÄ||¬≤ + Œ±¬∑||V||‚ÇÅ
```

Donde `||V||‚ÇÅ` es la norma L1 que promueve sparsity.

---

## Selecci√≥n de Componentes Principales

### ¬øCu√°ntos componentes retener?

**Pregunta clave:** De las `p` componentes calculadas, ¬øcu√°ntas usar?

### Criterio 1: Proporci√≥n de Varianza

**Regla del 80-90%:**
Retener suficientes componentes para explicar 80-90% de varianza total.

```python
cumsum = np.cumsum(pcs.explained_variance_ratio_)
k = np.argmax(cumsum >= 0.80) + 1
```

**Ejemplo:**
```
PC1: 45%  ‚Üí Acum: 45%
PC2: 28%  ‚Üí Acum: 73%
PC3: 15%  ‚Üí Acum: 88%  ‚Üê Retener hasta aqu√≠
PC4: 8%   ‚Üí Acum: 96%
PC5: 4%   ‚Üí Acum: 100%
```

**Por qu√©:**
- Las primeras PCs capturan la estructura principal
- Las √∫ltimas PCs suelen ser ruido
- 80-90% es un balance razonable

### Criterio 2: Scree Plot

Graficar valores propios (o varianza explicada) vs n√∫mero de componente.

```
Varianza |     ‚Ä¢
         |      
         |       ‚Ä¢
         |         
         |          ‚Ä¢  ‚Ä¢
         |___________‚Ä¢__‚Ä¢__‚Ä¢_
            1  2  3  4  5  6
         Componentes
```

**Buscar el "codo":**
- Donde la curva se aplana
- Antes del codo: informaci√≥n real
- Despu√©s del codo: ruido

### Criterio 3: Regla de Kaiser

**Retener componentes con valor propio > 1** (si datos est√°n escalados)

**Justificaci√≥n:**
- Si Œª·µ¢ > 1, PC·µ¢ explica m√°s varianza que una variable original
- Si Œª·µ¢ < 1, PC·µ¢ explica menos que una variable original (no vale la pena)

### Criterio 4: Validaci√≥n Cruzada

Probar diferentes valores de `k` en modelo posterior y evaluar performance.

```python
for k in range(1, p):
    X_reduced = pcs.transform(X)[:, :k]
    score = evaluate_model(X_reduced)
```

Elegir `k` con mejor score.

### ¬øPor qu√© funcionan las primeras 2-3 componentes?

**Razones matem√°ticas:**

1. **Teorema de Descomposici√≥n Espectral**
   - Los valores propios est√°n ordenados: Œª‚ÇÅ ‚â• Œª‚ÇÇ ‚â• ... ‚â• Œª‚Çö
   - Œª‚ÇÅ es la direcci√≥n de m√°xima varianza
   - Œª‚ÇÇ es la segunda direcci√≥n de m√°xima varianza, etc.

2. **Ley de Pareto (80/20)**
   - En datos reales, pocas variables explican mucho
   - La mayor parte de variaci√≥n est√° en pocas dimensiones

3. **Estructura de correlaci√≥n**
   - Variables correlacionadas ‚Üí informaci√≥n redundante
   - PCA elimina redundancia
   - Ejemplo: 10 variables con alta correlaci√≥n ‚Üí 2-3 PCs suficientes

4. **Maldici√≥n de la dimensionalidad**
   - M√°s dimensiones ‚Üí m√°s ruido
   - Dimensiones altas suelen ser ruido
   - Primeras PCs = se√±al, √∫ltimas PCs = ruido

**Analog√≠a:**
Imagina comprimir una imagen:
- Primeros componentes = contornos principales, colores dominantes
- √öltimos componentes = pixeles individuales, ruido
- Con 10% de componentes, recuperas 90% de la imagen

---

## Interpretaci√≥n de Resultados

### Loadings (Cargas)

**¬øC√≥mo interpretar?**

```
         PC1    PC2    PC3
calories  0.4    0.2   -0.1
protein   0.3    0.5    0.3
fat       0.5   -0.2    0.1
sugar    -0.4    0.3    0.6
fiber     0.3    0.6   -0.4
```

**Para PC1:**
- **Positivos altos** (fat: 0.5, calories: 0.4): "Altos en energ√≠a"
- **Negativos** (sugar: -0.4): "Bajos en az√∫car"
- **Interpretaci√≥n**: PC1 representa "Densidad energ√©tica sin az√∫car"

**Magnitud de loadings:**
- |loading| > 0.4: Variable muy importante
- 0.2 < |loading| < 0.4: Moderadamente importante
- |loading| < 0.2: Poco importante

**Signo de loadings:**
- Positivo: Variable aumenta con el componente
- Negativo: Variable disminuye con el componente

### Scores

**¬øC√≥mo interpretar?**

Un cereal con:
```
PC1_score = 3.5   (alto)
PC2_score = -1.2  (bajo)
PC3_score = 0.1   (neutro)
```

Interpretaci√≥n basada en PC1 y PC2:
- Alto en PC1 ‚Üí Alta densidad energ√©tica sin az√∫car
- Bajo en PC2 ‚Üí (interpretar seg√∫n loadings de PC2)

**Utilidad:**
- Comparar cereales en espacio reducido
- Identificar grupos similares
- Detectar anomal√≠as

### Biplot

Combina scores y loadings en un mismo gr√°fico:
- Puntos = observaciones (scores)
- Vectores = variables (loadings)

**Interpretaci√≥n:**
- Puntos cercanos ‚Üí observaciones similares
- Vectores largos ‚Üí variables con alta varianza
- Vectores en misma direcci√≥n ‚Üí variables correlacionadas positivamente
- Vectores en direcci√≥n opuesta ‚Üí correlaci√≥n negativa

---

## Sparse PCA

### Diferencias con PCA regular

| Aspecto | PCA Regular | Sparse PCA |
|---------|-------------|------------|
| Loadings | Todos ‚â† 0 | Muchos = 0 |
| Interpretaci√≥n | Dif√≠cil | F√°cil |
| Varianza explicada | M√°xima | Ligeramente menor |
| Costo computacional | Bajo | Alto |
| Uso | Reducci√≥n dimensi√≥n | Interpretaci√≥n + reducci√≥n |

### C√°lculo de varianza en Sparse PCA

```python
Q, R = np.linalg.qr(spca.components_)
R2 = np.diag(R) * np.diag(R)
```

**¬øPor qu√© QR?**
- Sparse PCA no garantiza ortogonalidad perfecta
- Descomposici√≥n QR ortogonaliza los componentes
- R¬≤ captura la varianza de cada componente ortogonalizado

**Ordenamiento:**
```python
arr1 = np.argsort(R2)[::-1]
R2_ = R2[arr1]
```
- Ordena componentes por varianza (mayor a menor)
- Similar a PCA regular

---

## Ejercicios de Estudio

### Ejercicio 1: Conceptual

**Pregunta:** Tienes un dataset con 100 observaciones y 50 variables. Despu√©s de PCA:
- PC1 explica 60% de varianza
- PC2 explica 20%
- PC3 explica 10%
- Resto: 10% distribuido en PC4-PC50

¬øCu√°ntos componentes retendr√≠as y por qu√©?

**An√°lisis:**
- 3 componentes: 90% de varianza ‚úÖ
- Resto parece ruido (10% en 47 componentes)
- Criterio 80-90%: 2-3 componentes

### Ejercicio 2: C√°lculo Manual

Dadas dos variables con matriz de covarianza:
```
S = | 4   2 |
    | 2   3 |
```

**Calcular:**
a) Valores propios (eigenvalues)
b) Vectores propios (eigenvectors)
c) Proporci√≥n de varianza de cada PC

**Soluci√≥n:**
```
det(S - ŒªI) = 0
(4-Œª)(3-Œª) - 4 = 0
Œª¬≤ - 7Œª + 8 = 0
Œª‚ÇÅ = 5.56, Œª‚ÇÇ = 1.44

PropVar‚ÇÅ = 5.56/7 = 79.4%
PropVar‚ÇÇ = 1.44/7 = 20.6%
```

### Ejercicio 3: Interpretaci√≥n

Dataset de estudiantes con variables:
```
- horas_estudio
- horas_sue√±o
- nota_final
- faltas
- participaci√≥n
```

Supongamos PC1 tiene loadings:
```
horas_estudio: 0.45
horas_sue√±o: 0.30
nota_final: 0.50
faltas: -0.40
participaci√≥n: 0.35
```

**Interpretar PC1:**
- Positivo: buen estudiante (estudia, duerme bien, buena nota, participa)
- Negativo: mal estudiante (no estudia, falta)
- PC1 = "√çndice de compromiso acad√©mico"

### Ejercicio 4: Escalamiento

**Pregunta:** ¬øQu√© sucede si NO escalamos variables con diferentes unidades?

Ejemplo:
```
ingreso:  [20000, 50000, 100000]  (varianza: 1.6e9)
edad:     [25, 35, 45]            (varianza: 100)
```

**Sin escalar:**
- PC1 ‚âà 0.999¬∑ingreso + 0.001¬∑edad
- Ingreso domina por varianza mayor

**Con escalar:**
- PC1 ‚âà 0.7¬∑ingreso + 0.7¬∑edad
- Ambas variables contribuyen equitativamente

### Ejercicio 5: Aplicaci√≥n

**Tarea:** Dise√±a un experimento PCA para:

Dataset: Informaci√≥n de casas
```
Variables: precio, √°rea, habitaciones, ba√±os, antig√ºedad, distancia_centro
```

**Pasos:**
1. ¬øEscalar√≠as las variables? ¬øPor qu√©?
2. ¬øCu√°ntos componentes esperar√≠as retener?
3. ¬øC√≥mo interpretar√≠as PC1 y PC2?
4. ¬øUsar√≠as PCA o Sparse PCA? Justifica.

**Respuesta sugerida:**
1. **S√≠ escalar**: diferentes unidades (m¬≤, a√±os, km, $)
2. **2-3 componentes**: variables probablemente correlacionadas
3. Interpretaci√≥n posible:
   - PC1: "Tama√±o/calidad general" (√°rea, habitaciones, ba√±os, precio)
   - PC2: "Ubicaci√≥n vs antig√ºedad" (distancia vs edad)
4. **PCA regular**: pocas variables, todas importantes (usar Sparse si hubiera 50+ variables)

---

## Preguntas Frecuentes

### 1. ¬øPCA es supervisado o no supervisado?

**No supervisado**: No usa etiquetas/clases. Solo busca estructura en X.

### 2. ¬øPCA elimina variables o las combina?

**Combina**: Crea nuevas variables (PCs) como combinaciones lineales de originales.

### 3. ¬øSe puede recuperar la informaci√≥n original?

**S√≠, parcialmente**: 
```python
X_aprox = scores ¬∑ loadings·µÄ
```
Si usamos todas las PCs, recuperaci√≥n perfecta. Si usamos k < p, aproximaci√≥n.

### 4. ¬øPCA asume distribuci√≥n normal?

**No necesariamente**: Funciona sin asumir normalidad, pero funciona mejor con datos aproximadamente normales.

### 5. ¬øQu√© hacer con valores faltantes?

**Opciones:**
1. Eliminar filas con NAs (como en c√≥digo: `dropna()`)
2. Imputar valores (media, mediana, KNN)
3. Usar variantes de PCA robustas a missing data

### 6. ¬øPCA maneja outliers bien?

**No muy bien**: Outliers pueden distorsionar los componentes. 
**Soluci√≥n**: Remover outliers o usar Robust PCA.

---

## Resumen de Conceptos Clave

### Matem√°ticos
- ‚úÖ PCA descompone matriz de covarianza en valores/vectores propios
- ‚úÖ Componentes principales = direcciones de m√°xima varianza
- ‚úÖ Componentes son ortogonales (no correlacionados)
- ‚úÖ Valores propios ordenados: Œª‚ÇÅ ‚â• Œª‚ÇÇ ‚â• ... ‚â• Œª‚Çö

### Pr√°cticos
- ‚úÖ Escalar datos si diferentes unidades
- ‚úÖ Retener componentes que expliquen 80-90% varianza
- ‚úÖ Primeras 2-3 PCs usualmente suficientes
- ‚úÖ Visualizar con scree plot y biplot

### Interpretaci√≥n
- ‚úÖ Loadings: c√≥mo se forma cada PC
- ‚úÖ Scores: posici√≥n de observaciones en nuevo espacio
- ‚úÖ Varianza explicada: importancia de cada PC
- ‚úÖ Sparse PCA: mejor interpretabilidad

### Limitaciones
- ‚ö†Ô∏è Solo para variables continuas
- ‚ö†Ô∏è Asume relaciones lineales
- ‚ö†Ô∏è Sensible a outliers
- ‚ö†Ô∏è Interpretaci√≥n puede ser dif√≠cil

---

## Referencias para Profundizar

### Libros
1. **"The Elements of Statistical Learning"** - Hastie, Tibshirani, Friedman
   - Cap√≠tulo 14: PCA y reducci√≥n de dimensionalidad

2. **"Pattern Recognition and Machine Learning"** - Christopher Bishop
   - Cap√≠tulo 12: PCA y an√°lisis factorial

3. **"Applied Multivariate Statistical Analysis"** - Johnson & Wichern
   - Cap√≠tulo 8: PCA

### Art√≠culos
- Jolliffe, I.T. (2002). "Principal Component Analysis"
- Zou, Hastie & Tibshirani (2006). "Sparse Principal Component Analysis"

### Recursos Online
- StatQuest: Videos explicativos de PCA
- Scikit-learn documentation: Ejemplos pr√°cticos

---

## Checklist de Estudio

Marca cuando domines cada concepto:

### Conceptos B√°sicos
- [ ] Definici√≥n de PCA
- [ ] Valores y vectores propios
- [ ] Matriz de covarianza
- [ ] Diferencia entre loadings y scores

### Aplicaci√≥n
- [ ] Cu√°ndo usar PCA
- [ ] Cu√°ndo NO usar PCA
- [ ] Necesidad de escalar datos
- [ ] Interpretar resultados de PCA

### Avanzado
- [ ] Selecci√≥n de n√∫mero de componentes
- [ ] Sparse PCA
- [ ] Variantes de PCA
- [ ] Limitaciones y alternativas

---

**¬°Buena suerte en tu estudio! üìäüéì**

