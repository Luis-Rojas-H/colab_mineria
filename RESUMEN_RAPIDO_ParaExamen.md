# RESUMEN RÃPIDO PARA EXAMEN - MINERÃA DE DATOS
## Repaso de 30 minutos antes del parcial

---

## ğŸ¯ MAPA CONCEPTUAL GENERAL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLASIFICACIÃ“N SUPERVISADA                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PROBABILÃSTICA   â”‚  BASADO EN     â”‚  ENSAMBLES      â”‚  â”‚
â”‚  â”‚                  â”‚  ÃRBOLES       â”‚                 â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Naive Bayes      â”‚ Decision Tree  â”‚ Random Forest   â”‚  â”‚
â”‚  â”‚                  â”‚ (CART)         â”‚                 â”‚  â”‚
â”‚  â”‚ â€¢ Usa Bayes      â”‚ â€¢ Usa Gini/IG  â”‚ â€¢ VotaciÃ³n      â”‚  â”‚
â”‚  â”‚ â€¢ RÃ¡pido         â”‚ â€¢ Interpretableâ”‚ â€¢ Robusto       â”‚  â”‚
â”‚  â”‚ â€¢ Discretos      â”‚ â€¢ Overfitting  â”‚ â€¢ Feature IM    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ NAIVE BAYES EN 60 SEGUNDOS

### La FÃ³rmula Clave
```
P(Clase | Datos) âˆ P(Datos | Clase) Ã— P(Clase)
```

### Â¿CuÃ¡ndo usarlo?
```
âœ… USAR                    âŒ NO USAR
- Datos discretos          - Variables continuas
- RÃ¡pido necesario         - Datos muy correlacionados
- Dataset pequeÃ±o          - Necesitas explicar "por quÃ©"
- ClasificaciÃ³n de texto   
```

### Multinomial vs Gaussian
```
MULTINOMIAL NAIVE BAYES:
â”œâ”€ Para: Datos categÃ³ricos/conteos
â”œâ”€ Ejemplo: Texto, dÃ­as de semana
â””â”€ En notebook: FlightDelays âœ…

GAUSSIAN NAIVE BAYES:
â”œâ”€ Para: Datos continuos
â”œâ”€ Ejemplo: Edad, ingresos
â””â”€ En notebook: âŒ No se usÃ³
```

### ParÃ¡metro Importante: Alpha (Laplace Smoothing)
```
alpha = 0.01  (en nuestro notebook)

Â¿Por quÃ©?
- Evita P(caracterÃ­stica) = 0
- Permite generalizar a casos nuevos
- Mayor alpha = mÃ¡s suavizado
```

### Tabla de Probabilidades en FlightDelays
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CARRIER      â”‚ P(Delay)     â”‚ P(OnTime)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Delta (DL)   â”‚ 0.0958       â”‚ 0.2040         â”‚
â”‚ AA           â”‚ 0.0575       â”‚ 0.0349         â”‚
â”‚ Southwest    â”‚ 0.0690       â”‚ 0.2181         â”‚
â”‚ US Airways   â”‚ Baja         â”‚ Alta           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

InterpretaciÃ³n: US Airways tiene mÃ¡s retrasos (probabilidad alta)
```

---

## 2ï¸âƒ£ ÃRBOLES DE DECISIÃ“N EN 60 SEGUNDOS

### Concepto
```
Ãrbol = Serie de preguntas SI/NO
â””â”€ Cada pregunta: Â¿CaracterÃ­stica > Valor?
â””â”€ Objetivo: Minimizar impureza
```

### MÃ©trica Clave: GINI
```
Gini = 1 - Î£(proporcionesÂ²)

Ejemplos:
- Nodo con [100 A, 0 B]:    Gini = 1 - (1Â² + 0Â²) = 0 (puro)
- Nodo con [50 A, 50 B]:    Gini = 1 - (0.5Â² + 0.5Â²) = 0.5 (impuro)
- Nodo con [60 A, 40 B]:    Gini = 1 - (0.6Â² + 0.4Â²) = 0.48 (impuro)

InterpretaciÃ³n: Gini bajo = nodo bueno âœ…
```

### Ganancia de InformaciÃ³n (Information Gain)
```
IG = Gini(Padre) - Media(Gini(Hijos))

Proceso:
1. Calcular Gini del nodo actual
2. Dividir por cada caracterÃ­stica
3. Calcular Gini de hijos
4. Elegir divisiÃ³n con mayor IG
```

### ParÃ¡metros CrÃ­ticos
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ max_depth          â”‚ Profundidad mÃ¡xima del Ã¡rbol     â”‚
â”‚ â””â”€ Sin lÃ­mite      â”‚ Memoriza datos (overfitting)     â”‚
â”‚ â””â”€ Con lÃ­mite=10   â”‚ Generaliza mejor                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ min_samples_split  â”‚ MÃ­nimas muestras para dividir    â”‚
â”‚ â””â”€ Baja (5)        â”‚ Demasiadas divisiones (ruido)    â”‚
â”‚ â””â”€ Alta (20)       â”‚ Menos divisiones (underfitting)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ min_impurity_...   â”‚ Solo divide si mejora bastante   â”‚
â”‚ â””â”€ 0.01            â”‚ Evita divisiones inÃºtiles        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### El Problema del Overfitting
```
Ãrbol SIN restricciones:
â”œâ”€ Training: 100% âœ…
â”œâ”€ Validation: 75% âŒ â† PROBLEMA
â””â”€ Causa: Memoriza datos de training

Ãrbol CON restricciones:
â”œâ”€ Training: 85%
â”œâ”€ Validation: 84% âœ… â† EXCELENTE
â””â”€ Causa: Generaliza mejor
```

---

## 3ï¸âƒ£ RANDOM FOREST EN 60 SEGUNDOS

### Concepto
```
Random Forest = 500 Ã¡rboles votando

Proceso:
1. Tomar 500 muestras aleatorias (bootstrap)
2. Entrenar 1 Ã¡rbol por cada muestra
3. Para predecir: votaciÃ³n mayoritaria
4. Resultado: predicciÃ³n robusta
```

### Ventaja Principal
```
Reduce OVERFITTING automÃ¡ticamente
â””â”€ Ãrbol solo: Muy propenso a memorizar
â””â”€ Random Forest: Muy robusto
```

### Feature Importance
```
Â¿CuÃ¡l es la caracterÃ­stica mÃ¡s importante?

En UniversalBank:
1. Income:        0.3338 (33.38%) â† La mÃ¡s importante
2. Education:     0.2008 (20.08%)
3. CCAvg:         0.1721 (17.21%)
4. Family:        0.1114 (11.14%)
5. Age:           0.0363 (3.63%)   â† Menos importante

InterpretaciÃ³n: Income explica 33% de la predicciÃ³n
```

---

## 4ï¸âƒ£ VARIABLES DISCRETAS vs CONTINUAS

### Tabla RÃ¡pida
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tipo de Modelo   â”‚ Discretas    â”‚ Continuas        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MultinomialNB    â”‚ âœ… Perfecto  â”‚ âŒ Malo          â”‚
â”‚ GaussianNB       â”‚ âŒ Malo      â”‚ âœ… Perfecto      â”‚
â”‚ DecisionTree     â”‚ âœ… Excelente â”‚ âœ… Excelente     â”‚
â”‚ RandomForest     â”‚ âœ… Excelente â”‚ âœ… Excelente     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿Por quÃ© Multinomial NB NECESITA discretas?
```
Si x es CONTINUA (ej: edad=34.567):
- Casi nunca se repite exactamente
- Conteos = 0 o 1 (no confiable)
- P(edad=34.567 | retraso) = imposible calcular

SOLUCIÃ“N: Binning/CategorizaciÃ³n
edad = 34.567 â†’ edad_30_40 = 1 (variable binaria)
Ahora: P(edad_30_40 | retraso) = fÃ¡cil calcular âœ…
```

### CÃ³mo Convertir
```python
# OpciÃ³n 1: Binning manual
df['edad_categoria'] = pd.cut(df['edad'], 
                               bins=[0, 30, 40, 50, 100])

# OpciÃ³n 2: One-hot encoding (como en FlightDelays)
X = pd.get_dummies(df[['DAY_WEEK', 'CARRIER']])
# Resultado: DAY_WEEK_1, DAY_WEEK_2, ..., CARRIER_DL, etc.

# OpciÃ³n 3: Label encoding (MALO para NB)
df['aerolÃ­nea_numero'] = df['CARRIER'].map({'AA': 0, 'DL': 1})
# âŒ Problema: Asume orden (1 > 0)
```

---

## 5ï¸âƒ£ SELECCIÃ“N DE CARACTERÃSTICAS

### Cuatro Pasos
```
1. Â¿CUÃLES SÃ usar?
   â””â”€ Tienen relaciÃ³n lÃ³gica con objetivo
   â””â”€ InformaciÃ³n disponible en tiempo de predicciÃ³n
   â””â”€ Tienen poder predictivo

2. Â¿CUÃLES NO usar?
   â””â”€ Identificadores (ID, ZIP Code)
   â””â”€ InformaciÃ³n futura (no disponible)
   â””â”€ Muy correlacionadas con otra

3. Â¿CÃ“MO ELEGIRLAS?
   â””â”€ CorrelaciÃ³n con objetivo
   â””â”€ Feature Importance de Ã¡rboles
   â””â”€ Mutual Information

4. Â¿CUÃNTAS?
   â””â”€ Menos = modelo mÃ¡s simple y rÃ¡pido
   â””â”€ MÃ¡s = mejor precisiÃ³n (hasta cierto punto)
```

### En Nuestros Notebooks

**FlightDelays - Multinomial NB:**
```python
predictors = ['DAY_WEEK', 'CRS_DEP_TIME', 'ORIGIN', 'DEST', 'CARRIER']

Â¿POR QUÃ‰?
âœ… Disponibles ANTES del vuelo
âœ… Todas categÃ³ricas/discretas
âœ… Tienen poder predictivo
âœ… FÃ¡ciles de obtener

Â¿POR QUÃ‰ NO otras?
âŒ DEP_TIME: Solo disponible despuÃ©s
âŒ Weather: PodrÃ­a usarse pero no estaba
âŒ TAIL_NUM: Muy especÃ­fica del aviÃ³n
```

**UniversalBank - Decision Trees:**
```
âœ… Todas las columnas numÃ©ricas (Age, Income, etc.)
âŒ EXCLUIDAS: ID, ZIP Code, Personal Loan

Ranked por importancia:
1. Income (0.334) â†’ Capacidad de pago
2. Education (0.201) â†’ Ingresos futuros
3. CCAvg (0.172) â†’ Comportamiento
4. Family (0.111) â†’ Necesidades
5. Experience (0.036) â†’ Estabilidad
```

---

## 6ï¸âƒ£ MATRIZ DE CONFUSIÃ“N RÃPIDA

### El Cuadro
```
                  Predicho
             Positive  Negative
Real Positive  TP      FN
Real Negative  FP      TN

Ejemplo: PredicciÃ³n de retrasos
                  Predicho
             Delay  OnTime
Real Delay    45      10  â† EncontrÃ© 45 retrasos
Real OnTime   15     280  â† Predije que 280 llegarÃ­an a tiempo
```

### MÃ©tricas de 30 Segundos
```
Accuracy   = (TP+TN)/(Total) â† Porcentaje correcto (general)
Precision  = TP/(TP+FP)      â† De mis predicciones delay, Â¿cuÃ¡ntas correctas?
Recall     = TP/(TP+FN)      â† De los retrasos reales, Â¿cuÃ¡ntos detectÃ©?
```

### En nuestros notebooks
```
FlightDelays (Multinomial NB):
â””â”€ Accuracy: 80.48% âœ… (bastante bueno)

UniversalBank (Random Forest):
â””â”€ Accuracy: 98.20% âœ… (excelente)
```

---

## 7ï¸âƒ£ VALIDACIÃ“N CRUZADA (CRUCIAL)

### Â¿Por quÃ©?
```
SIN Cross-Validation:
â”œâ”€ Split random train/test
â”œâ”€ Resultado depende de la suerte
â””â”€ No confiable âŒ

CON 5-Fold CV:
â”œâ”€ 5 divisiones diferentes
â”œâ”€ Promedio de 5 resultados
â””â”€ Robusto y confiable âœ…
```

### VisualizaciÃ³n
```
Dataset total = 1000 muestras

Fold 1: Train [200-1000] Test [0-200]     Scoreâ‚ = 0.85
Fold 2: Train [0-200, 400-1000] Test [200-400]   Scoreâ‚‚ = 0.84
Fold 3: Train [0-400, 600-1000] Test [400-600]   Scoreâ‚ƒ = 0.86
Fold 4: Train [0-600, 800-1000] Test [600-800]   Scoreâ‚„ = 0.85
Fold 5: Train [0-800] Test [800-1000]            Scoreâ‚… = 0.84

PROMEDIO = (0.85 + 0.84 + 0.86 + 0.85 + 0.84) / 5 = 0.848 âœ…
```

---

## 8ï¸âƒ£ GRIDSEARCHCV - Encontrar Mejores ParÃ¡metros

### El Concepto
```
Probar todas las combinaciones de parÃ¡metros
y elegir la mejor

Ejemplo del notebook:
param_grid = {
    'max_depth': [10, 20, 30, 40],           (4 valores)
    'min_samples_split': [20, 40, 60, ...],  (5 valores)
    'min_impurity_decrease': [0, 0.0005, ...] (5 valores)
}

Combinaciones = 4 Ã— 5 Ã— 5 = 100 â† probar 100 modelos diferentes
```

### Resultado
```
Mejor score encontrado: 0.9877 (98.77%)
Mejores parÃ¡metros: {
    'max_depth': 4,
    'min_impurity_decrease': 0.0011,
    'min_samples_split': 13
}
```

---

## 9ï¸âƒ£ FÃ“RMULAS PARA MEMORIZAR

```
PROBABILIDAD POSTERIOR (Naive Bayes):
P(y|X) âˆ P(X|y) Ã— P(y)

GINI (Ãrbol):
Gini = 1 - Î£â±¼ pâ±¼Â²

INFORMACIÃ“N GAIN (Ãrbol):
IG = Gini_padre - Media(Gini_hijos)

LAPLACE SMOOTHING (Naive Bayes):
P = (conteos + Î±) / (total + Î±Ã—V)

ACCURACY:
Accuracy = (TP + TN) / (TP + TN + FP + FN)

PRECISION:
Precision = TP / (TP + FP)

RECALL:
Recall = TP / (TP + FN)

F1-SCORE:
F1 = 2 Ã— (Precision Ã— Recall) / (P + R)
```

---

## ğŸ”Ÿ TABLA COMPARATIVA FINAL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Criterio            â”‚ NB Multinomialâ”‚ Decision Treeâ”‚Random Forest â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tipo de datos ideal â”‚ Discretos    â”‚ Mixtos       â”‚ Mixtos       â”‚
â”‚ Velocidad train     â”‚ âš¡âš¡âš¡ RÃ¡pido â”‚ âš¡âš¡ Medio   â”‚ ğŸ¢ Lento     â”‚
â”‚ Interpretabilidad   â”‚ â­â­â­ Alta  â”‚ â­â­â­â­ Muy â”‚ â­â­ Media   â”‚
â”‚ Overfitting risk    â”‚ âœ… Bajo      â”‚ âŒ Alto      â”‚ âœ… Muy bajo  â”‚
â”‚ Feature Importance  â”‚ âŒ No        â”‚ âœ… SÃ­        â”‚ âœ… Excelente â”‚
â”‚ Comple interaccionesâ”‚ âŒ No        â”‚ âœ… SÃ­        â”‚ âœ… SÃ­        â”‚
â”‚ Datos continuos     â”‚ âŒ Pobre     â”‚ âœ… Excelente â”‚ âœ… Excelente â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ PREGUNTAS MÃS PROBABLES EN EXAMEN

### P1: Â¿Por quÃ© usar variables DISCRETAS en Naive Bayes?
```
RESPUESTA:
Porque MultinomialNB calcula P(x|y) = (conteos)/(total)

Si x es continua (ej: 34.567), no se repite exactamente
â†’ conteos = 0 Ã³ 1 â†’ estimaciÃ³n no confiable

SoluciÃ³n: Convertir a discreta (binning o one-hot encoding)
```

### P2: Â¿CuÃ¡l es la diferencia entre GINI e INFORMACIÃ“N GAIN?
```
RESPUESTA:
- Gini: Mide IMPUREZA de un nodo (0=puro, 0.5=impuro)
- Information Gain: REDUCCIÃ“N en impureza tras dividir
- Se elige la divisiÃ³n con mayor IG (mayor reducciÃ³n)
```

### P3: Â¿CÃ³mo EVITAR OVERFITTING en Ã¡rboles?
```
RESPUESTA:
1. Limitar profundidad (max_depth)
2. Requerir mÃ­nimas muestras para dividir (min_samples_split)
3. Requerir mejora significativa (min_impurity_decrease)
4. Usar Random Forest (reduce automÃ¡ticamente)
5. ValidaciÃ³n cruzada (medir generalizaciÃ³n)
```

### P4: Â¿CuÃ¡ndo es MEJOR Random Forest que un solo Ã¡rbol?
```
RESPUESTA:
- Cuando queremos mejor GENERALIZACIÃ“N
- Cuando queremos REDUCIR VARIANZA
- Cuando el dataset es GRANDE
- Cuando NO necesitamos mÃ¡xima INTERPRETABILIDAD
- VENTAJA: 98% de precisiÃ³n tÃ­picamente
- DESVENTAJA: Menos interpretable
```

### P5: Â¿QuÃ© es LAPLACE SMOOTHING y por quÃ© usarlo?
```
RESPUESTA:
- Suma Î± (alpha, tÃ­picamente 1 o 0.01) al numerador y denominador
- Evita probabilidades CERO
- Permite generalizar a nuevas observaciones
- En Naive Bayes: alpha=0.01 en nuestro notebook
```

---

## âš ï¸ ERRORES COMUNES EN EXAMEN

```
âŒ NUNCA:
- Confundir Gini con InformaciÃ³n Gain
- Usar discretas en Gaussian NB
- Usar continuas directamente en Multinomial NB
- Olvidar dividir datos en train/test
- Comparar modelos sin Cross-Validation

âœ… SIEMPRE:
- Verificar tipo de variable (discreta/continua)
- Elegir algoritmo segÃºn datos
- Usar train/test split (70/30 o 80/20)
- Validar con Cross-Validation
- Reportar mÃºltiples mÃ©tricas (Accuracy, Precision, Recall)
```

---

## ğŸ“ CHECKLIST ANTES DEL EXAMEN

```
â–¡ Â¿Entiendo por quÃ© Naive Bayes usa discretas?
â–¡ Â¿SÃ© calcular Gini manualmente?
â–¡ Â¿Entiendo cÃ³mo funciona Decision Tree?
â–¡ Â¿SÃ© quÃ© es overfitting y cÃ³mo evitarlo?
â–¡ Â¿Conozco los parÃ¡metros clave (max_depth, min_samples)?
â–¡ Â¿Entiendo Feature Importance en Random Forest?
â–¡ Â¿Puedo leer una Matriz de ConfusiÃ³n?
â–¡ Â¿SÃ© cuÃ¡ndo usar cada algoritmo?
â–¡ Â¿Entiendo One-Hot Encoding?
â–¡ Â¿Conozco Cross-Validation?
```

---

## ğŸš€ ACCIONES FINALES ANTES DEL EXAMEN

```
5 MINUTOS ANTES:
1. Repasa las fÃ³rmulas clave (estÃ¡n arriba)
2. Repasa cuÃ¡ndo usar cada algoritmo
3. Recuerda: NB = discretas, Tree = cualquiera

DURANTE EL EXAMEN:
1. Lee bien las preguntas
2. Dibuja Ã¡rboles/matrices si es necesario
3. Muestra desarrollo matemÃ¡tico
4. Explica por quÃ© elegiste cada algoritmo

SI NO SABES UNA PREGUNTA:
1. No entres en pÃ¡nico
2. Escribe lo que sÃ­ sepas
3. Haz conexiones con conceptos relacionados
```

---

**Â¡Ã‰XITO EN TU EXAMEN! ğŸ‰**

*Documento: Resumen RÃ¡pido para Examen*  
*MinerÃ­a de Datos - 8vo Ciclo*
