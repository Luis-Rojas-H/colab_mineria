{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä EXAMEN FINAL - MINER√çA DE DATOS\n",
        "## Soluci√≥n Completa Paso a Paso\n",
        "\n",
        "**Fecha:** 2025  \n",
        "**Nivel:** Universitario  \n",
        "**M√©todos:** Clustering, Collaborative Filtering, Association Rules\n",
        "\n",
        "---\n",
        "\n",
        "## Problemas a Resolver:\n",
        "\n",
        "1. **Pharmaceutical Industry** - An√°lisis de Clustering (K-Means)\n",
        "2. **Recommending Courses** - User-based Collaborative Filtering  \n",
        "3. **Satellite Radio Customers** - An√°lisis de Association Rules\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PARTE 0: INSTALACI√ìN E IMPORTACIONES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  √ó Building wheel for scikit-surprise (pyproject.toml) did not run successfully.\n",
            "  ‚îÇ exit code: 1\n",
            "  ‚ï∞‚îÄ> [155 lines of output]\n",
            "      C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
            "      \n",
            "              By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
            "              or your builds will no longer be supported.\n",
            "      \n",
            "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        corresp(dist, value, root_dir)\n",
            "      C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "      \n",
            "              License :: OSI Approved :: BSD License\n",
            "      \n",
            "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        dist._finalize_license_expression()\n",
            "      C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
            "      \n",
            "              License :: OSI Approved :: BSD License\n",
            "      \n",
            "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        self._finalize_license_expression()\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
            "      copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
            "      copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
            "      copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
            "      copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
            "      creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      running egg_info\n",
            "      writing scikit_surprise.egg-info\\PKG-INFO\n",
            "      writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
            "      writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
            "      writing requirements to scikit_surprise.egg-info\\requires.txt\n",
            "      writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
            "      dependency C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
            "      reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no previously-included files matching '*.so' found under directory 'surprise'\n",
            "      adding license file 'LICENSE.md'\n",
            "      writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
            "      C:\\Users\\USUARIO\\AppData\\Local\\Temp\\pip-build-env-1lb6lyp4\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              ############################\n",
            "              # Package would be ignored #\n",
            "              ############################\n",
            "              Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
            "              but it is absent from setuptools' `packages` configuration.\n",
            "      \n",
            "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "              package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
            "              to the `packages` configuration field.\n",
            "      \n",
            "              Alternatively, you can also rely on setuptools' discovery methods\n",
            "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "              instead of `find_packages(...)`/`find:`).\n",
            "      \n",
            "              You can read more about \"package discovery\" on setuptools documentation page:\n",
            "      \n",
            "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "      \n",
            "              If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
            "              already explicitly excluding 'surprise.prediction_algorithms' via\n",
            "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "              combination with a more fine grained `package-data` configuration.\n",
            "      \n",
            "              You can read more about \"package data files\" on setuptools documentation page:\n",
            "      \n",
            "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "      \n",
            "      \n",
            "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "                    even if it does not contain any `.py` files.\n",
            "                    On the other hand, currently there is no concept of package data\n",
            "                    directory, all directories are treated like packages.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        check.warn(importable)\n",
            "      copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
            "      copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
            "      running build_ext\n",
            "      building 'surprise.similarities' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for scikit-surprise\n",
            "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Instalaci√≥n de librer√≠as necesarias\n",
        "%pip install scikit-learn pandas numpy matplotlib seaborn scipy mlxtend surprise -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones principales\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import pairwise\n",
        "from pandas.plotting import parallel_coordinates\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "print(\"‚úì Todas las librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PROBLEMA 1: PHARMACEUTICAL INDUSTRY\n",
        "## An√°lisis de Clustering\n",
        "\n",
        "**Objetivo:** Entender la estructura de la industria farmac√©utica usando medidas financieras b√°sicas mediante an√°lisis de clustering.\n",
        "\n",
        "**Dataset:** pharmaceuticals.csv (21 empresas farmac√©uticas)\n",
        "\n",
        "**Variables:**\n",
        "- **Num√©ricas (1-9):** Market Cap, Beta, PE Ratio, ROE, ROA, Asset Turnover, Leverage, Rev Growth, Net Profit Margin\n",
        "- **Categ√≥ricas (10-12):** Median Recommendation, Location, Exchange\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.a) Clustering usando solo variables num√©ricas (1-9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos de farmac√©uticas\n",
        "pharma_df = pd.read_csv('Pharmaceuticals.csv')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"INFORMACI√ìN DEL DATASET\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nDimensiones: {pharma_df.shape}\")\n",
        "print(f\"\\nColumnas disponibles:\")\n",
        "print(pharma_df.columns.tolist())\n",
        "print(f\"\\nPrimeras 5 filas:\")\n",
        "print(pharma_df.head())\n",
        "print(f\"\\nTipos de datos:\")\n",
        "print(pharma_df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar variables num√©ricas (1-9) y categ√≥ricas (10-12)\n",
        "# Variables num√©ricas seg√∫n el problema:\n",
        "numerical_vars = ['Market_Cap', 'Beta', 'PE_Ratio', 'ROE', 'ROA', \n",
        "                  'Asset_Turnover', 'Leverage', 'Rev_Growth', 'Net_Profit_Margin']\n",
        "\n",
        "# Variables categ√≥ricas (10-12):\n",
        "categorical_vars = ['Median_Recommendation', 'Location', 'Exchange']\n",
        "\n",
        "# Establecer Symbol (o Name) como √≠ndice\n",
        "if 'Name' in pharma_df.columns:\n",
        "    pharma_df.set_index('Name', inplace=True)\n",
        "elif 'Symbol' in pharma_df.columns:\n",
        "    pharma_df.set_index('Symbol', inplace=True)\n",
        "\n",
        "print(\"Variables num√©ricas para clustering:\", numerical_vars)\n",
        "print(\"\\nVariables categ√≥ricas para interpretaci√≥n:\", categorical_vars)\n",
        "print(f\"\\nDataset con √≠ndice establecido:\")\n",
        "print(pharma_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datos num√©ricos para clustering\n",
        "X_pharma = pharma_df[numerical_vars].copy()\n",
        "\n",
        "# Verificar valores faltantes\n",
        "print(\"Valores faltantes por variable:\")\n",
        "print(X_pharma.isnull().sum())\n",
        "print(f\"\\nTotal de valores faltantes: {X_pharma.isnull().sum().sum()}\")\n",
        "\n",
        "# Remover filas con valores faltantes si existen\n",
        "if X_pharma.isnull().sum().sum() > 0:\n",
        "    X_pharma = X_pharma.dropna()\n",
        "    print(f\"\\n‚úì Filas despu√©s de remover NaN: {len(X_pharma)}\")\n",
        "\n",
        "print(f\"\\nEstad√≠sticas descriptivas:\")\n",
        "print(X_pharma.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JUSTIFICACI√ìN: Normalizaci√≥n de variables\n",
        "# Dado que las variables tienen diferentes escalas (Market_Cap en billones, \n",
        "# ratios porcentuales, etc.), es CR√çTICO normalizar para que todas tengan \n",
        "# el mismo peso en el clustering\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"JUSTIFICACI√ìN DE NORMALIZACI√ìN\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úì Market_Cap: en billones de d√≥lares (rango: 1-200)\")\n",
        "print(\"‚úì ROE, ROA: porcentajes (rango: 5-30)\")\n",
        "print(\"‚úì Beta: ratios (rango: 0.2-1.5)\")\n",
        "print(\"\\n‚Üí Sin normalizaci√≥n, Market_Cap dominar√≠a el clustering\")\n",
        "print(\"‚Üí Con normalizaci√≥n (z-score), todas las variables tienen peso igual\")\n",
        "\n",
        "# Normalizar usando z-score (standardization)\n",
        "X_pharma_norm = X_pharma.apply(preprocessing.scale, axis=0)\n",
        "X_pharma_norm.columns = X_pharma.columns  # Mantener nombres\n",
        "\n",
        "print(f\"\\n‚úì Datos normalizados (z-score)\")\n",
        "print(f\"‚úì Media de cada variable ‚âà 0, Desviaci√≥n ‚âà 1\")\n",
        "print(f\"\\nVerificaci√≥n (medias deber√≠an ser ~0):\")\n",
        "print(X_pharma_norm.mean().round(6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JUSTIFICACI√ìN: Selecci√≥n del n√∫mero de clusters (k)\n",
        "# Usaremos el m√©todo del \"elbow\" (codo) para encontrar el k √≥ptimo\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SELECCI√ìN DEL N√öMERO DE CLUSTERS (M√âTODO ELBOW)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calcular inercia para diferentes valores de k\n",
        "inertias = []\n",
        "k_range = range(2, 8)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_pharma_norm)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Visualizar el m√©todo elbow\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('N√∫mero de Clusters (k)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Inercia (Within-Cluster Sum of Squares)', fontsize=12, fontweight='bold')\n",
        "plt.title('M√©todo Elbow para Selecci√≥n de k', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_range)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Mostrar valores\n",
        "elbow_df = pd.DataFrame({'k': k_range, 'Inercia': inertias})\n",
        "print(\"\\nInercia por n√∫mero de clusters:\")\n",
        "print(elbow_df)\n",
        "\n",
        "# Identificar el \"codo\" (donde la reducci√≥n de inercia se estabiliza)\n",
        "print(\"\\n‚úì An√°lisis: Buscamos el punto donde la reducci√≥n de inercia se estabiliza\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar k basado en an√°lisis del elbow\n",
        "# Para este an√°lisis, usaremos k=4 o k=5 (depende del codo observado)\n",
        "# Por ahora usaremos k=4 como valor razonable\n",
        "\n",
        "optimal_k = 4  # Ajustar seg√∫n el gr√°fico del elbow\n",
        "\n",
        "print(f\"=\" * 70)\n",
        "print(f\"CLUSTERING K-MEANS CON k={optimal_k}\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nJUSTIFICACI√ìN DEL N√öMERO DE CLUSTERS:\")\n",
        "print(f\"‚Ä¢ k={optimal_k} proporciona un balance entre:\")\n",
        "print(f\"  - Cohesi√≥n dentro de clusters (baja inercia)\")\n",
        "print(f\"  - Separaci√≥n entre clusters\")\n",
        "print(f\"  - Interpretabilidad (no demasiados grupos)\")\n",
        "\n",
        "# Aplicar K-Means\n",
        "kmeans_pharma = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans_pharma.fit(X_pharma_norm)\n",
        "\n",
        "# Asignar clusters a cada empresa\n",
        "pharma_clusters = pd.Series(kmeans_pharma.labels_, index=X_pharma_norm.index, name='Cluster')\n",
        "pharma_df_with_clusters = pharma_df.copy()\n",
        "pharma_df_with_clusters['Cluster'] = pharma_clusters\n",
        "\n",
        "print(f\"\\n‚úì Modelo K-Means entrenado con k={optimal_k}\")\n",
        "print(f\"‚úì Clusters asignados a {len(pharma_clusters)} empresas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostrar composici√≥n de cada cluster\n",
        "print(\"=\" * 70)\n",
        "print(\"COMPOSICI√ìN DE CLUSTERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for cluster_id in sorted(pharma_clusters.unique()):\n",
        "    cluster_firms = pharma_clusters[pharma_clusters == cluster_id].index.tolist()\n",
        "    print(f\"\\nCluster {cluster_id} ({len(cluster_firms)} empresas):\")\n",
        "    print(f\"  {', '.join(cluster_firms)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.b) Interpretaci√≥n de clusters con respecto a variables categ√≥ricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar patrones en variables categ√≥ricas por cluster\n",
        "print(\"=\" * 70)\n",
        "print(\"AN√ÅLISIS DE VARIABLES CATEG√ìRICAS POR CLUSTER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Agregar clusters al dataframe original\n",
        "for cat_var in categorical_vars:\n",
        "    if cat_var in pharma_df_with_clusters.columns:\n",
        "        print(f\"\\n{cat_var} por Cluster:\")\n",
        "        print(\"-\" * 70)\n",
        "        crosstab = pd.crosstab(pharma_df_with_clusters['Cluster'], \n",
        "                               pharma_df_with_clusters[cat_var], \n",
        "                               margins=True)\n",
        "        print(crosstab)\n",
        "        print(f\"\\nProporciones:\")\n",
        "        prop_tab = pd.crosstab(pharma_df_with_clusters['Cluster'], \n",
        "                               pharma_df_with_clusters[cat_var], \n",
        "                               normalize='index') * 100\n",
        "        print(prop_tab.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.c) Patrones en variables num√©ricas no usadas en clustering\n",
        "\n",
        "**Nota:** El problema pregunta sobre variables 10-12, pero estas son categ√≥ricas. \n",
        "Asumimos que se refiere a analizar si hay patrones en las variables categ√≥ricas \n",
        "respecto a los clusters formados (ya hecho en 1.b), o si hay alguna otra variable \n",
        "num√©rica adicional. Procederemos a analizar los centroides de los clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analizar centroides de clusters (valores promedio normalizados)\n",
        "centroids = pd.DataFrame(kmeans_pharma.cluster_centers_, \n",
        "                         columns=X_pharma_norm.columns,\n",
        "                         index=[f'Cluster {i}' for i in range(optimal_k)])\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CENTROIDES DE CLUSTERS (valores normalizados)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nInterpretaci√≥n:\")\n",
        "print(\"‚Ä¢ Valores > 0: Por encima del promedio general\")\n",
        "print(\"‚Ä¢ Valores < 0: Por debajo del promedio general\")\n",
        "print(\"‚Ä¢ Valores cerca de 0: Cerca del promedio general\")\n",
        "print(\"\\nCentroides:\")\n",
        "print(centroids.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar perfiles de clusters usando parallel coordinates\n",
        "centroids_viz = centroids.copy()\n",
        "centroids_viz['Cluster'] = centroids_viz.index\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "parallel_coordinates(centroids_viz, 'Cluster', colormap='tab10', linewidth=3)\n",
        "plt.title('Perfiles de Clusters - Pharmaceutical Industry', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Variables Financieras', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Valor Normalizado (z-score)', fontsize=12, fontweight='bold')\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úì Gr√°fico de coordenadas paralelas generado\")\n",
        "print(\"‚Üí Permite visualizar el perfil caracter√≠stico de cada cluster\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.d) Nombres apropiados para cada cluster\n",
        "\n",
        "Bas√°ndonos en los centroides y las variables categ√≥ricas, asignaremos nombres descriptivos a cada cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis detallado para nombrar clusters\n",
        "print(\"=\" * 70)\n",
        "print(\"AN√ÅLISIS PARA NOMBRAR CLUSTERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calcular promedios por cluster en escala original (desnormalizada)\n",
        "cluster_profiles = pharma_df_with_clusters.groupby('Cluster')[numerical_vars].mean()\n",
        "\n",
        "print(\"\\nPromedios por Cluster (valores originales):\")\n",
        "print(cluster_profiles.round(2))\n",
        "\n",
        "# Analizar caracter√≠sticas distintivas\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CARACTER√çSTICAS DISTINTIVAS POR CLUSTER:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for cluster_id in sorted(pharma_clusters.unique()):\n",
        "    cluster_data = pharma_df_with_clusters[pharma_df_with_clusters['Cluster'] == cluster_id]\n",
        "    cluster_profile = cluster_profiles.loc[cluster_id]\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CLUSTER {cluster_id}:\")\n",
        "    print(f\"Empresas: {', '.join(cluster_data.index.tolist())}\")\n",
        "    print(f\"\\nCaracter√≠sticas Financieras:\")\n",
        "    print(f\"  Market Cap promedio: ${cluster_profile['Market_Cap']:.2f} billones\")\n",
        "    print(f\"  ROE promedio: {cluster_profile['ROE']:.2f}%\")\n",
        "    print(f\"  ROA promedio: {cluster_profile['ROA']:.2f}%\")\n",
        "    print(f\"  Beta promedio: {cluster_profile['Beta']:.2f}\")\n",
        "    print(f\"  Leverage promedio: {cluster_profile['Leverage']:.2f}\")\n",
        "    print(f\"  Rev Growth promedio: {cluster_profile['Rev_Growth']:.2f}%\")\n",
        "    \n",
        "    if 'Location' in cluster_data.columns:\n",
        "        print(f\"\\nLocations: {cluster_data['Location'].value_counts().to_dict()}\")\n",
        "    if 'Exchange' in cluster_data.columns:\n",
        "        print(f\"Exchanges: {cluster_data['Exchange'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Asignar nombres descriptivos a clusters\n",
        "cluster_names = {}\n",
        "for cluster_id in sorted(pharma_clusters.unique()):\n",
        "    cluster_profile = cluster_profiles.loc[cluster_id]\n",
        "    \n",
        "    # Determinar caracter√≠sticas dominantes\n",
        "    if cluster_profile['Market_Cap'] > cluster_profiles['Market_Cap'].median():\n",
        "        size = \"Grandes\"\n",
        "    else:\n",
        "        size = \"Peque√±as/Medianas\"\n",
        "    \n",
        "    if cluster_profile['ROE'] > cluster_profiles['ROE'].median():\n",
        "        profitability = \"Alta Rentabilidad\"\n",
        "    else:\n",
        "        profitability = \"Rentabilidad Media/Baja\"\n",
        "    \n",
        "    if cluster_profile['Rev_Growth'] > cluster_profiles['Rev_Growth'].median():\n",
        "        growth = \"Alto Crecimiento\"\n",
        "    else:\n",
        "        growth = \"Crecimiento Moderado\"\n",
        "    \n",
        "    # Combinar para crear nombre\n",
        "    if cluster_profile['Market_Cap'] > 50:\n",
        "        name = f\"{size} Capitalizadas - {profitability}\"\n",
        "    else:\n",
        "        name = f\"{size} - {profitability} - {growth}\"\n",
        "    \n",
        "    cluster_names[cluster_id] = name\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"NOMBRES ASIGNADOS A CLUSTERS\")\n",
        "print(\"=\" * 70)\n",
        "for cluster_id, name in cluster_names.items():\n",
        "    print(f\"\\nCluster {cluster_id}: {name}\")\n",
        "    cluster_firms = pharma_clusters[pharma_clusters == cluster_id].index.tolist()\n",
        "    print(f\"  Empresas: {', '.join(cluster_firms)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PROBLEMA 2: RECOMMENDING COURSES\n",
        "## User-based Collaborative Filtering\n",
        "\n",
        "**Objetivo:** Aplicar user-based collaborative filtering para recomendar cursos a un estudiante que compr√≥ \"Regression\" y \"Forecast\".\n",
        "\n",
        "**Dataset:** CourseTopics.csv (compras de cursos en Statistics.com)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar librer√≠as para collaborative filtering\n",
        "from surprise import Dataset, Reader, KNNBasic\n",
        "from surprise.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import heapq\n",
        "\n",
        "print(\"‚úì Librer√≠as para collaborative filtering importadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos de cursos\n",
        "courses_df = pd.read_csv('Coursetopics.csv')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"DATASET DE CURSOS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nDimensiones: {courses_df.shape}\")\n",
        "print(f\"\\nCursos disponibles: {courses_df.columns.tolist()}\")\n",
        "print(f\"\\nPrimeras 10 filas:\")\n",
        "print(courses_df.head(10))\n",
        "print(f\"\\nTotal de estudiantes: {len(courses_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertir formato: de matriz ancha (wide) a formato largo (long) para Surprise\n",
        "# Surprise requiere: user_id, item_id, rating\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CONVERSI√ìN A FORMATO PARA COLLABORATIVE FILTERING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Crear formato largo\n",
        "course_ratings = []\n",
        "for student_id in range(len(courses_df)):\n",
        "    for course in courses_df.columns:\n",
        "        rating = courses_df.iloc[student_id][course]\n",
        "        if rating == 1:  # Solo cursos comprados (rating = 1)\n",
        "            course_ratings.append({\n",
        "                'userID': student_id,\n",
        "                'itemID': course,\n",
        "                'rating': 1\n",
        "            })\n",
        "\n",
        "ratings_df = pd.DataFrame(course_ratings)\n",
        "\n",
        "print(f\"\\nFormato largo creado:\")\n",
        "print(f\"Total de registros: {len(ratings_df)}\")\n",
        "print(f\"Usuarios √∫nicos: {ratings_df['userID'].nunique()}\")\n",
        "print(f\"Cursos √∫nicos: {ratings_df['itemID'].nunique()}\")\n",
        "print(f\"\\nPrimeras filas:\")\n",
        "print(ratings_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar el estudiante objetivo (que compr√≥ Regression y Forecast)\n",
        "print(\"=\" * 70)\n",
        "print(\"IDENTIFICAR ESTUDIANTE OBJETIVO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Buscar estudiantes que tienen Regression Y Forecast\n",
        "target_students = courses_df[\n",
        "    (courses_df['Regression'] == 1) & \n",
        "    (courses_df['Forecast'] == 1)\n",
        "]\n",
        "\n",
        "print(f\"\\nEstudiantes que compraron Regression Y Forecast: {len(target_students)}\")\n",
        "print(f\"\\n√çndices de estudiantes objetivo: {target_students.index.tolist()}\")\n",
        "\n",
        "if len(target_students) > 0:\n",
        "    target_student_id = target_students.index[0]  # Tomar el primero\n",
        "    print(f\"\\n‚úì Usaremos estudiante ID: {target_student_id}\")\n",
        "    print(f\"\\nCursos que este estudiante ya compr√≥:\")\n",
        "    student_courses = courses_df.iloc[target_student_id]\n",
        "    purchased = student_courses[student_courses == 1].index.tolist()\n",
        "    print(f\"  {', '.join(purchased)}\")\n",
        "else:\n",
        "    print(\"\\n‚ö† No se encontr√≥ estudiante con ambos cursos\")\n",
        "    target_student_id = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intentar aplicar User-based Collaborative Filtering\n",
        "print(\"=\" * 70)\n",
        "print(\"APLICAR USER-BASED COLLABORATIVE FILTERING\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    # Preparar datos para Surprise\n",
        "    reader = Reader(rating_scale=(0, 1))  # Ratings binarios: 0 o 1\n",
        "    data = Dataset.load_from_df(ratings_df[['userID', 'itemID', 'rating']], reader)\n",
        "    \n",
        "    # Construir trainset completo\n",
        "    trainset = data.build_full_trainset()\n",
        "    \n",
        "    # Configurar algoritmo user-based\n",
        "    sim_options = {'name': 'cosine', 'user_based': True}\n",
        "    algo = KNNBasic(sim_options=sim_options, verbose=False)\n",
        "    \n",
        "    # Entrenar modelo\n",
        "    algo.fit(trainset)\n",
        "    \n",
        "    print(\"\\n‚úì Modelo entrenado\")\n",
        "    print(f\"‚úì N√∫mero de usuarios: {trainset.n_users}\")\n",
        "    print(f\"‚úì N√∫mero de items: {trainset.n_items}\")\n",
        "    \n",
        "    # Intentar predecir para el estudiante objetivo\n",
        "    if target_student_id is not None:\n",
        "        all_courses = courses_df.columns.tolist()\n",
        "        purchased_courses = courses_df.iloc[target_student_id][courses_df.iloc[target_student_id] == 1].index.tolist()\n",
        "        unpurchased_courses = [c for c in all_courses if c not in purchased_courses]\n",
        "        \n",
        "        print(f\"\\nPredicciones para estudiante {target_student_id}:\")\n",
        "        predictions = []\n",
        "        for course in unpurchased_courses:\n",
        "            try:\n",
        "                pred = algo.predict(target_student_id, course)\n",
        "                predictions.append((course, pred.est))\n",
        "            except:\n",
        "                predictions.append((course, None))\n",
        "        \n",
        "        if any(p[1] is None for p in predictions):\n",
        "            print(\"\\n‚ö† ADVERTENCIA: Algunas predicciones son None\")\n",
        "        else:\n",
        "            predictions_sorted = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
        "            print(\"\\nRecomendaciones (ordenadas por rating predicho):\")\n",
        "            for course, rating in predictions_sorted[:5]:\n",
        "                print(f\"  {course}: {rating:.4f}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERROR al aplicar collaborative filtering:\")\n",
        "    print(f\"   {str(e)}\")\n",
        "    print(f\"\\n   Tipo de error: {type(e).__name__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANALIZAR POR QU√â SE OBTIENE UNA MATRIZ NULL\n",
        "print(\"=\" * 70)\n",
        "print(\"EXPLICACI√ìN: ¬øPOR QU√â SE OBTIENE UNA MATRIZ NULL?\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nAN√ÅLISIS DEL PROBLEMA:\")\n",
        "print(\"\\n1. DATOS BINARIOS (0/1):\")\n",
        "print(\"   ‚Ä¢ Los datos son binarios: 1 = curso comprado, 0 = no comprado\")\n",
        "print(\"   ‚Ä¢ Collaborative filtering t√≠picamente usa ratings continuos (1-5)\")\n",
        "\n",
        "print(\"\\n2. MATRIZ DE SIMILITUD DE USUARIOS:\")\n",
        "print(\"   ‚Ä¢ User-based CF calcula similitud entre usuarios\")\n",
        "print(\"   ‚Ä¢ Si dos usuarios NO comparten ning√∫n curso, similitud = 0 o indefinida\")\n",
        "print(\"   ‚Ä¢ Con datos sparse (pocos 1s), muchos usuarios no tienen overlap\")\n",
        "\n",
        "print(\"\\n3. PROBLEMA DE SPARSITY (ESPARCIDAD):\")\n",
        "overlap_matrix = np.zeros((len(courses_df), len(courses_df)))\n",
        "for i in range(len(courses_df)):\n",
        "    for j in range(len(courses_df)):\n",
        "        if i != j:\n",
        "            # Calcular overlap (cursos compartidos)\n",
        "            overlap = (courses_df.iloc[i] * courses_df.iloc[j]).sum()\n",
        "            overlap_matrix[i, j] = overlap\n",
        "\n",
        "print(f\"\\n   ‚Ä¢ Matriz de overlap entre usuarios calculada\")\n",
        "print(f\"   ‚Ä¢ Usuarios con overlap > 0: {(overlap_matrix > 0).sum()}\")\n",
        "print(f\"   ‚Ä¢ Total de pares: {len(courses_df) * (len(courses_df) - 1)}\")\n",
        "print(f\"   ‚Ä¢ Sparsity: {(overlap_matrix == 0).sum() / (len(courses_df) * (len(courses_df) - 1)) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\n4. RESULTADO:\")\n",
        "print(\"   ‚Ä¢ Sin suficiente overlap, no se pueden calcular similitudes confiables\")\n",
        "print(\"   ‚Ä¢ La matriz de similitud resultante tiene muchos valores NULL/NaN\")\n",
        "print(\"   ‚Ä¢ Por tanto, no se pueden hacer predicciones confiables\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CONCLUSI√ìN\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úì La matriz NULL ocurre porque:\")\n",
        "print(\"   1. Los datos son demasiado sparse (pocos cursos comprados por usuario)\")\n",
        "print(\"   2. Hay poco overlap entre usuarios (pocos cursos compartidos)\")\n",
        "print(\"   3. User-based CF requiere usuarios similares con preferencias compartidas\")\n",
        "print(\"   4. Con datos binarios y sparse, es dif√≠cil encontrar vecinos similares\")\n",
        "print(\"\\n‚úì SOLUCIONES ALTERNATIVAS:\")\n",
        "print(\"   ‚Ä¢ Item-based collaborative filtering (puede funcionar mejor)\")\n",
        "print(\"   ‚Ä¢ Content-based filtering (usar caracter√≠sticas de cursos)\")\n",
        "print(\"   ‚Ä¢ Association rules (encontrar reglas: Regression + Forecast ‚Üí ?)\")\n",
        "print(\"   ‚Ä¢ Reducir threshold de similitud m√≠nima requerida\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PROBLEMA 3: SATELLITE RADIO CUSTOMERS\n",
        "## Comentarios sobre Association Rules\n",
        "\n",
        "**Situaci√≥n:** Un analista quiere aplicar association rules para encontrar grupos de clientes asociados entre s√≠ usando datos demogr√°ficos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis del enfoque propuesto\n",
        "print(\"=\" * 70)\n",
        "print(\"AN√ÅLISIS: APLICAR ASSOCIATION RULES A DATOS DEMOGR√ÅFICOS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nDATOS DISPONIBLES (seg√∫n Tabla 14.13):\")\n",
        "print(\"  ‚Ä¢ Zipconvert_2, Zipconvert_3, Zipconvert_4, Zipconvert_5 (Dummy)\")\n",
        "print(\"  ‚Ä¢ Homeowner (Dummy)\")\n",
        "print(\"  ‚Ä¢ NUMCHLS (n√∫mero de hijos)\")\n",
        "print(\"  ‚Ä¢ INCOME (nivel de ingresos)\")\n",
        "print(\"  ‚Ä¢ gender (g√©nero)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMENTARIOS SOBRE EL ENFOQUE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n1. ASSOCIATION RULES NO ES APROPIADO PARA ENCONTRAR 'GRUPOS DE CLIENTES'\")\n",
        "print(\"   ‚Ä¢ Association Rules encuentra relaciones entre ITEMS/CARACTER√çSTICAS\")\n",
        "print(\"   ‚Ä¢ Ejemplo: {Homeowner=1, Income=High} ‚Üí {Purchase=Premium}\")\n",
        "print(\"   ‚Ä¢ NO encuentra grupos de clientes similares entre s√≠\")\n",
        "\n",
        "print(\"\\n2. QU√â ES APROPIADO PARA ENCONTRAR GRUPOS DE CLIENTES:\")\n",
        "print(\"   ‚úì CLUSTERING (K-Means, Hierarchical Clustering)\")\n",
        "print(\"     ‚Üí Agrupa clientes similares en clusters\")\n",
        "print(\"     ‚Üí Basado en caracter√≠sticas demogr√°ficas similares\")\n",
        "print(\"   ‚úì SEGMENTACI√ìN DE CLIENTES\")\n",
        "print(\"     ‚Üí Divide clientes en grupos homog√©neos\")\n",
        "print(\"     ‚Üí Cada grupo tiene caracter√≠sticas demogr√°ficas similares\")\n",
        "\n",
        "print(\"\\n3. CU√ÅNDO USAR ASSOCIATION RULES:\")\n",
        "print(\"   ‚úì Para encontrar relaciones entre CARACTER√çSTICAS\")\n",
        "print(\"     Ejemplo: 'Clientes con hijos tienden a ser homeowners'\")\n",
        "print(\"   ‚úì Para encontrar reglas de COMPRA/PREFERENCIAS\")\n",
        "print(\"     Ejemplo: 'Si compr√≥ X, entonces probablemente compre Y'\")\n",
        "print(\"   ‚úì Para an√°lisis de MARKET BASKET (cesta de compras)\")\n",
        "print(\"     Ejemplo: 'Leche y pan se compran juntos frecuentemente'\")\n",
        "\n",
        "print(\"\\n4. PROBLEMA CON DATOS CONTINUOS:\")\n",
        "print(\"   ‚Ä¢ NUMCHLS e INCOME son variables num√©ricas\")\n",
        "print(\"   ‚Ä¢ Association Rules t√≠picamente requiere datos binarios/categ√≥ricos\")\n",
        "print(\"   ‚Ä¢ Necesitar√≠an discretizaci√≥n/binning primero\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"RECOMENDACI√ìN\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚úì USAR CLUSTERING en lugar de Association Rules:\")\n",
        "print(\"   1. Preparar datos (normalizar variables continuas)\")\n",
        "print(\"   2. Aplicar K-Means o Clustering Jer√°rquico\")\n",
        "print(\"   3. Interpretar clusters seg√∫n caracter√≠sticas demogr√°ficas\")\n",
        "print(\"   4. Nombrar cada segmento de clientes\")\n",
        "print(\"\\n‚úì SI QUIERE USAR ASSOCIATION RULES:\")\n",
        "print(\"   1. Discretizar variables continuas (ej: INCOME ‚Üí Low/Medium/High)\")\n",
        "print(\"   2. Convertir todo a formato binario (one-hot encoding)\")\n",
        "print(\"   3. Aplicar Apriori para encontrar itemsets frecuentes\")\n",
        "print(\"   4. Generar reglas de asociaci√≥n\")\n",
        "print(\"   5. PERO esto encontrar√° relaciones entre caracter√≠sticas,\")\n",
        "print(\"      NO grupos de clientes similares\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo visual de la diferencia\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EJEMPLO: DIFERENCIA ENTRE CLUSTERING Y ASSOCIATION RULES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nCLUSTERING (Correcto para grupos de clientes):\")\n",
        "print(\"  Input: Caracter√≠sticas demogr√°ficas de clientes\")\n",
        "print(\"  Output: Grupos de clientes similares\")\n",
        "print(\"  Ejemplo:\")\n",
        "print(\"    Cluster 1: [Cliente 17, Cliente 40, Cliente 53]\")\n",
        "print(\"    ‚Üí Todos: Homeowner=1, Income=High, Zip=3\")\n",
        "print(\"  ‚Üí Encuentra QU√â CLIENTES son similares\")\n",
        "\n",
        "print(\"\\nASSOCIATION RULES (Incorrecto para grupos de clientes):\")\n",
        "print(\"  Input: Transacciones o caracter√≠sticas\")\n",
        "print(\"  Output: Reglas de asociaci√≥n entre caracter√≠sticas\")\n",
        "print(\"  Ejemplo:\")\n",
        "print(\"    Regla: {Homeowner=1, Income=High} ‚Üí {Zipconvert_3=1}\")\n",
        "print(\"    Support: 0.3, Confidence: 0.8\")\n",
        "print(\"  ‚Üí Encuentra QU√â CARACTER√çSTICAS est√°n relacionadas\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CONCLUSI√ìN FINAL\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n‚ùå Association Rules NO es el m√©todo apropiado para encontrar\")\n",
        "print(\"   grupos de clientes asociados entre s√≠.\")\n",
        "print(\"\\n‚úÖ Clustering (K-Means, Hierarchical) S√ç es el m√©todo apropiado.\")\n",
        "print(\"\\n‚úÖ Association Rules ser√≠a apropiado para encontrar relaciones\")\n",
        "print(\"   entre caracter√≠sticas demogr√°ficas, NO entre clientes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# RESUMEN DEL EXAMEN\n",
        "\n",
        "## Problema 1: Pharmaceutical Industry\n",
        "- ‚úì Clustering K-Means aplicado con k=4\n",
        "- ‚úì Variables num√©ricas normalizadas para igual peso\n",
        "- ‚úì Clusters interpretados seg√∫n variables categ√≥ricas\n",
        "- ‚úì Nombres descriptivos asignados a cada cluster\n",
        "\n",
        "## Problema 2: Recommending Courses\n",
        "- ‚úì User-based collaborative filtering intentado\n",
        "- ‚úì Explicaci√≥n de por qu√© se obtiene matriz NULL:\n",
        "  - Datos demasiado sparse\n",
        "  - Poco overlap entre usuarios\n",
        "  - Dificultad para encontrar vecinos similares\n",
        "\n",
        "## Problema 3: Satellite Radio Customers\n",
        "- ‚úì An√°lisis cr√≠tico del enfoque propuesto\n",
        "- ‚úì Association Rules NO es apropiado para grupos de clientes\n",
        "- ‚úì Clustering es el m√©todo correcto para este objetivo\n",
        "\n",
        "---\n",
        "\n",
        "**Fin del Examen Final - Miner√≠a de Datos**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
