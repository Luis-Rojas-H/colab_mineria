# EJERCICIOS PR√ÅCTICOS CON SOLUCIONES
## Miner√≠a de Datos - 8vo Ciclo

---

## EJERCICIO 1: Calcular Gini Manualmente

### Problema
Un dataset tiene 100 muestras en un nodo:
- 70 muestras de Clase A (Retraso)
- 30 muestras de Clase B (OnTime)

**Calcular Gini**

### Soluci√≥n
```
p_A = 70/100 = 0.7
p_B = 30/100 = 0.3

Gini = 1 - (p_A¬≤ + p_B¬≤)
Gini = 1 - (0.7¬≤ + 0.3¬≤)
Gini = 1 - (0.49 + 0.09)
Gini = 1 - 0.58
Gini = 0.42

Interpretaci√≥n: El nodo es moderadamente puro (Gini=0.42)
```

---

## EJERCICIO 2: Calcular Information Gain

### Problema
Un nodo PADRE tiene:
- 100 muestras: 70 Clase A, 30 Clase B
- Gini_padre = 0.42 (del ejercicio anterior)

Se divide por caracter√≠stica X en dos HIJOS:
- Hijo 1: 60 muestras (50 A, 10 B) ‚Üí Gini_1 = 0.333
- Hijo 2: 40 muestras (20 A, 20 B) ‚Üí Gini_2 = 0.5

**Calcular Information Gain**

### Soluci√≥n
```
Gini_Hijo1 = 1 - (50/60)¬≤ - (10/60)¬≤
           = 1 - 0.6944¬≤ - 0.1667¬≤
           = 1 - 0.4823 - 0.0278
           = 0.4899 ‚âà 0.333 (verificado) ‚úì

Gini_Hijo2 = 1 - (20/40)¬≤ - (20/40)¬≤
           = 1 - 0.5¬≤ - 0.5¬≤
           = 1 - 0.25 - 0.25
           = 0.5 ‚úì

IG = Gini_padre - [w‚ÇÅ√óGini_hijo1 + w‚ÇÇ√óGini_hijo2]

donde:
w‚ÇÅ = 60/100 = 0.6 (peso hijo 1)
w‚ÇÇ = 40/100 = 0.4 (peso hijo 2)

IG = 0.42 - [0.6√ó0.333 + 0.4√ó0.5]
IG = 0.42 - [0.1998 + 0.2]
IG = 0.42 - 0.3998
IG = 0.0202

Interpretaci√≥n: La divisi√≥n reduce impureza en 0.0202
(peque√±a ganancia, no es una buena divisi√≥n)
```

---

## EJERCICIO 3: Aplicar Teorema de Bayes

### Problema
En FlightDelays, queremos predecir Flight Status para un vuelo con:
- CARRIER = DL (Delta)
- DAY_WEEK = 7 (Domingo)
- CRS_DEP_TIME = 15 (3 PM)

Del dataset de entrenamiento obtuvimos:
- P(Retraso) = 0.20 (20% de vuelos tienen retraso)
- P(OnTime) = 0.80 (80% llegan a tiempo)

Y las probabilidades condicionales:
- P(CARRIER_DL | Retraso) = 0.095
- P(CARRIER_DL | OnTime) = 0.204
- P(DAY_WEEK_7 | Retraso) = 0.161
- P(DAY_WEEK_7 | OnTime) = 0.105
- P(CRS_DEP_TIME_15 | Retraso) = 0.050
- P(CRS_DEP_TIME_15 | OnTime) = 0.080

**¬øCu√°l es la probabilidad de que sea un retraso?**

### Soluci√≥n
```
P(Retraso | DL, Domingo, 3PM) ‚àù P(DL|Retraso) √ó P(Domingo|Retraso) 
                                √ó P(3PM|Retraso) √ó P(Retraso)

Numerador_Retraso = 0.095 √ó 0.161 √ó 0.050 √ó 0.20
                  = 0.095 √ó 0.161 √ó 0.050 √ó 0.20
                  = 0.0001523

P(OnTime | DL, Domingo, 3PM) ‚àù P(DL|OnTime) √ó P(Domingo|OnTime) 
                               √ó P(3PM|OnTime) √ó P(OnTime)

Numerador_OnTime = 0.204 √ó 0.105 √ó 0.080 √ó 0.80
                 = 0.204 √ó 0.105 √ó 0.080 √ó 0.80
                 = 0.0013689

Normalizar:
P(Retraso | datos) = 0.0001523 / (0.0001523 + 0.0013689)
                   = 0.0001523 / 0.0015212
                   = 0.10 (10%)

P(OnTime | datos) = 0.0013689 / 0.0015212
                  = 0.90 (90%)

PREDICCI√ìN: OnTime (ya que 90% > 10%)
Confianza: 90%
```

---

## EJERCICIO 4: Matriz de Confusi√≥n

### Problema
En UniversalBank, usamos Random Forest para predecir "Personal Loan"

Predicciones en el conjunto de validaci√≥n (2000 muestras):
```
              Real
           Prestamo No-Prestamo
Predicho:
Prestamo      160        4
No-Prestamo   32       1804
```

**Calcular todas las m√©tricas**

### Soluci√≥n
```
TP (Verdaderos Positivos): 160  ‚Üê Predijo S√≠, era S√≠
FP (Falsos Positivos):     4    ‚Üê Predijo S√≠, era No
FN (Falsos Negativos):     32   ‚Üê Predijo No, era S√≠
TN (Verdaderos Negativos): 1804 ‚Üê Predijo No, era No

Total = 160 + 4 + 32 + 1804 = 2000

ACCURACY = (TP + TN) / Total
         = (160 + 1804) / 2000
         = 1964 / 2000
         = 0.982 = 98.2% ‚úÖ Muy bueno

PRECISION = TP / (TP + FP)
          = 160 / (160 + 4)
          = 160 / 164
          = 0.976 = 97.6%
          ‚Üê De mis predicciones de "S√≠ Pr√©stamo", 97.6% eran correctas

RECALL (SENSIBILIDAD) = TP / (TP + FN)
                      = 160 / (160 + 32)
                      = 160 / 192
                      = 0.833 = 83.3%
                      ‚Üê De los que realmente pidieron pr√©stamo, detect√© 83.3%

F1-SCORE = 2 √ó (Precision √ó Recall) / (Precision + Recall)
         = 2 √ó (0.976 √ó 0.833) / (0.976 + 0.833)
         = 2 √ó 0.813 / 1.809
         = 1.626 / 1.809
         = 0.899 = 89.9%

ESPECIFICIDAD = TN / (TN + FP)
              = 1804 / (1804 + 4)
              = 1804 / 1808
              = 0.998 = 99.8%
              ‚Üê De los que NO pidieron pr√©stamo, predije correctamente 99.8%

INTERPRETACI√ìN:
- Accuracy muy alta (98.2%) ‚Üí Modelo muy bueno
- Precision alta (97.6%) ‚Üí Pocas falsas alarmas
- Recall bueno (83.3%) ‚Üí Detecta 83% de solicitudes reales
- Especificidad excelente (99.8%) ‚Üí Casi no predice falsos positivos
```

---

## EJERCICIO 5: One-Hot Encoding

### Problema
Tenemos un dataset con:
```
ID  CARRIER  DAY_WEEK
1   AA       2
2   DL       5
3   AA       2
4   UA       7
```

**Aplicar One-Hot Encoding a CARRIER y DAY_WEEK**

### Soluci√≥n
```python
import pandas as pd

df = pd.DataFrame({
    'ID': [1, 2, 3, 4],
    'CARRIER': ['AA', 'DL', 'AA', 'UA'],
    'DAY_WEEK': [2, 5, 2, 7]
})

X = pd.get_dummies(df[['CARRIER', 'DAY_WEEK']])
```

**Resultado:**
```
   CARRIER_AA  CARRIER_DL  CARRIER_UA  DAY_WEEK_2  DAY_WEEK_5  DAY_WEEK_7
0           1           0           0           1           0           0
1           0           1           0           0           1           0
2           1           0           0           1           0           0
3           0           0           1           0           0           1
```

**Interpretaci√≥n:**
- Fila 0: AA, D√≠a 2 ‚Üí AA=1, DL=0, UA=0, D√≠a2=1, D√≠a5=0, D√≠a7=0
- Fila 1: DL, D√≠a 5 ‚Üí AA=0, DL=1, UA=0, D√≠a2=0, D√≠a5=1, D√≠a7=0
- etc.

**Ventaja:** Cada categor√≠a es una variable binaria, sin asumir orden
```

---

## EJERCICIO 6: Cross-Validation Manual

### Problema
Tenemos 10 muestras y queremos hacer 2-Fold Cross-Validation

Dataset:
```
[d1, d2, d3, d4, d5, d6, d7, d8, d9, d10]
Clases: [A,  A,  A,  B,  B,  A,  B,  A,  B,  A]
```

Entrenar DecisionTree y reportar accuracy de cada fold

### Soluci√≥n
```
FOLD 1:
‚îú‚îÄ Train: [d1, d2, d3, d4, d5] (clases: A, A, A, B, B)
‚îú‚îÄ Test:  [d6, d7, d8, d9, d10] (clases: A, B, A, B, A)
‚îú‚îÄ Entrenar √°rbol con train
‚îú‚îÄ Predecir test: [A, B, A, B, A]  ‚Üê Perfect match!
‚îî‚îÄ Accuracy_1 = 5/5 = 1.0 = 100%

FOLD 2:
‚îú‚îÄ Train: [d6, d7, d8, d9, d10] (clases: A, B, A, B, A)
‚îú‚îÄ Test:  [d1, d2, d3, d4, d5] (clases: A, A, A, B, B)
‚îú‚îÄ Entrenar √°rbol con train
‚îú‚îÄ Predecir test: [A, A, A, B, B]  ‚Üê Perfect match!
‚îî‚îÄ Accuracy_2 = 5/5 = 1.0 = 100%

PROMEDIO = (100% + 100%) / 2 = 100%
DESVIACI√ìN EST√ÅNDAR = 0%

Conclusi√≥n: Modelo generaliz√≥ perfectamente (datos muy simples)
```

---

## EJERCICIO 7: GridSearchCV Simplificado

### Problema
Queremos encontrar los mejores par√°metros para DecisionTree

Grid a probar:
```python
param_grid = {
    'max_depth': [1, 2, 3],
    'min_samples_split': [2, 4]
}
```

Dado Dataset con 100 muestras, 5-Fold CV

**Enumerar todas las combinaciones a probar**

### Soluci√≥n
```
Combinaciones posibles = 3 √ó 2 = 6 modelos

COMBINACI√ìN 1: max_depth=1, min_samples_split=2
‚îú‚îÄ 5-Fold CV en training
‚îî‚îÄ Score promedio = 0.75

COMBINACI√ìN 2: max_depth=1, min_samples_split=4
‚îú‚îÄ 5-Fold CV en training
‚îî‚îÄ Score promedio = 0.78

COMBINACI√ìN 3: max_depth=2, min_samples_split=2
‚îú‚îÄ 5-Fold CV en training
‚îî‚îÄ Score promedio = 0.82 ‚Üê Mejor hasta ahora

COMBINACI√ìN 4: max_depth=2, min_samples_split=4
‚îú‚îÄ 5-Fold CV en training
‚îî‚îÄ Score promedio = 0.80

COMBINACI√ìN 5: max_depth=3, min_samples_split=2
‚îú‚îÄ 5-Fold CV en training
‚îî‚îÄ Score promedio = 0.81

COMBINACI√ìN 6: max_depth=3, min_samples_split=4
‚îú‚îÄ 5-Fold CV en training
‚îî‚îÄ Score promedio = 0.79

MEJOR COMBINACI√ìN: max_depth=2, min_samples_split=2 con score 0.82 ‚úÖ

Entrenar modelo final con estos par√°metros en TODO el training
Evaluar en TEST set
```

---

## EJERCICIO 8: Seleccionar Caracter√≠sticas por Correlaci√≥n

### Problema
Dataset con 5 caracter√≠sticas y objetivo = Personal Loan

Correlaciones calculadas:
```
Income              0.45
Age                 0.32
Experience          0.30
Education           0.15
CC Avg              0.08
```

**Seleccionar top-3 caracter√≠sticas**

### Soluci√≥n
```
Ranking por correlaci√≥n (descendente):
1. Income:      0.45 ‚Üê #1 (mayor correlaci√≥n)
2. Age:         0.32 ‚Üê #2
3. Experience:  0.30 ‚Üê #3
4. Education:   0.15 ‚Üê No usar (muy baja)
5. CC Avg:      0.08 ‚Üê No usar (muy baja)

CARACTER√çSTICAS SELECCIONADAS: [Income, Age, Experience]

Justificaci√≥n:
- Income tiene la correlaci√≥n m√°s fuerte (0.45)
- Age y Experience son moderadamente correlacionadas
- Education y CC Avg tienen correlaci√≥n muy d√©bil (<0.15)

Interpretaci√≥n:
- Income explica 45% de la relaci√≥n con Personal Loan
- Age explica 32%
- Experience explica 30%
- El resto contribuye poco
```

---

## EJERCICIO 9: Identificar Overfitting

### Problema
Dos modelos entrenados, resultados:

**Modelo A:**
- Training Accuracy: 85%
- Validation Accuracy: 84%

**Modelo B:**
- Training Accuracy: 99%
- Validation Accuracy: 72%

**¬øCu√°l tiene overfitting?**

### Soluci√≥n
```
MODELO A:
‚îú‚îÄ Diferencia = 85% - 84% = 1% ‚úÖ
‚îú‚îÄ Peque√±a diferencia
‚îú‚îÄ Training y Validation similares
‚îî‚îÄ CONCLUSI√ìN: Sin overfitting (bueno)

MODELO B:
‚îú‚îÄ Diferencia = 99% - 72% = 27% ‚ùå
‚îú‚îÄ ENORME diferencia
‚îú‚îÄ Training perfecto, Validation pobre
‚îî‚îÄ CONCLUSI√ìN: SEVERO OVERFITTING

INTERPRETACI√ìN MODELO B:
- El modelo MEMORIZ√ì datos de training (99%)
- NO GENERALIZA a datos nuevos (72%)
- Necesita:
  1. Aumentar max_depth
  2. Aumentar min_samples_split
  3. Usar Random Forest
  4. M√°s data de training
```

---

## EJERCICIO 10: Feature Importance Ranking

### Problema
Un modelo Random Forest reporta:

```python
importances = [0.15, 0.35, 0.02, 0.28, 0.20]
features = ['Age', 'Income', 'Experience', 'Education', 'Family']
```

**Crear ranking y interpretar**

### Soluci√≥n
```
Crear DataFrame:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Feature    ‚îÇ Importance ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Income     ‚îÇ 0.35       ‚îÇ ‚Üê #1 (35%)
‚îÇ Education  ‚îÇ 0.28       ‚îÇ ‚Üê #2 (28%)
‚îÇ Family     ‚îÇ 0.20       ‚îÇ ‚Üê #3 (20%)
‚îÇ Age        ‚îÇ 0.15       ‚îÇ ‚Üê #4 (15%)
‚îÇ Experience ‚îÇ 0.02       ‚îÇ ‚Üê #5 (2%)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Total = 0.35 + 0.28 + 0.20 + 0.15 + 0.02 = 1.00 ‚úì

INTERPRETACI√ìN:
- Income es 17.5√ó m√°s importante que Experience (0.35 / 0.02)
- Las 3 primeras explican 83% de las predicciones
- Experience pr√°cticamente no influye (2%)

RECOMENDACI√ìN:
- Mantener: Income, Education, Family, Age
- Considerar eliminar: Experience (muy baja importancia)
- Reduce dimensionalidad sin perder precisi√≥n
```

---

## EJERCICIO 11: Problema de Pr√°ctica - Completo

### Situaci√≥n
La empresa "MejorBanco" te pide clasificar si otorgar un pr√©stamo personal.

Dataset: 1000 clientes, 80% train, 20% test

Caracter√≠sticas disponibles:
- Age (continua): 21-70 a√±os
- Salary (continua): $20k-$150k
- Credit_Score (continua): 300-850
- Employment_Years (continua): 0-40 a√±os
- Marital_Status (discreta): Single, Married, Divorced
- Education (discreta): High School, Bachelor, Master
- Phone_Ownership (discreta): Yes, No
- Has_Credit_Card (discreta): Yes, No
- Target: Loan_Approved (Si/No)

**Preguntas:**

a) ¬øQu√© algoritmo recomendar√≠as y por qu√©?
b) ¬øQu√© caracter√≠sticas excluir√≠as? ¬øPor qu√©?
c) ¬øNecesitar√≠as transformar alguna variable?
d) ¬øC√≥mo evaluar√≠as el modelo?

### Soluci√≥n

```
a) ALGORITMO RECOMENDADO: Random Forest

   JUSTIFICACI√ìN:
   ‚úÖ Datos mixtos (continuas + discretas)
   ‚úÖ Random Forest maneja ambas sin transformaci√≥n
   ‚úÖ Reduce overfitting autom√°ticamente
   ‚úÖ Proporciona Feature Importance
   ‚úÖ Alta precisi√≥n esperada (~95%)
   ‚ùå Decision Tree simple: Propenso a overfitting
   ‚ùå Naive Bayes: Necesitar√≠a muchas transformaciones

b) CARACTER√çSTICAS A EXCLUIR: Ninguna

   AN√ÅLISIS DE CADA CARACTER√çSTICA:
   ‚úÖ Age: Relevante (edad ‚Üí riesgo de cr√©dito)
   ‚úÖ Salary: MUY RELEVANTE (capacidad de pago)
   ‚úÖ Credit_Score: MUY RELEVANTE (historial crediticio)
   ‚úÖ Employment_Years: Relevante (estabilidad laboral)
   ‚úÖ Marital_Status: Relevante (dependientes)
   ‚úÖ Education: Relevante (ingresos futuros)
   ‚úÖ Phone_Ownership: Tal vez d√©bil, pero no perjudica
   ‚úÖ Has_Credit_Card: Indicador de historial

c) TRANSFORMACIONES NECESARIAS:

   Para Random Forest: NINGUNA requerida
   (ya maneja continuas y discretas)

   Pero si us√°ramos Naive Bayes (MultinomialNB):
   ‚îú‚îÄ Age ‚Üí Binning: Age_18_30, Age_30_45, Age_45_60, Age_60plus
   ‚îú‚îÄ Salary ‚Üí Binning: Salary_Low, Salary_Medium, Salary_High
   ‚îú‚îÄ Credit_Score ‚Üí Binning: Poor, Fair, Good, Excellent
   ‚îú‚îÄ Employment_Years ‚Üí Similar binning
   ‚îî‚îÄ Discretas: One-hot encoding

d) EVALUACI√ìN DEL MODELO:

   M√âTRICAS PRINCIPALES:
   ‚îú‚îÄ Accuracy: Porcentaje total correcto
   ‚îú‚îÄ Precision: De los pr√©stamos aprobados, ¬øcu√°ntos s√≠ pagaron?
   ‚îÇ  (Evitar falsas aprobaciones = cr√©ditos incobrables)
   ‚îú‚îÄ Recall: De los buenos pagadores, ¬øcu√°ntos detectamos?
   ‚îÇ  (No perder buenos clientes)
   ‚îî‚îÄ F1-Score: Balance entre ambos

   PROCESO:
   1. Dividir: 80% train, 20% test
   2. Entrenar Random Forest en train
   3. 5-Fold CV en train para validar
   4. Evaluar en test (sin data leakage)
   5. Matriz de confusi√≥n
   6. Feature Importance

   M√âTRICAS ACEPTABLES:
   ‚îú‚îÄ Accuracy: > 85%
   ‚îú‚îÄ Precision: > 80% (evitar falsas aprobaciones)
   ‚îú‚îÄ Recall: > 75% (capturar buenos clientes)
   ‚îî‚îÄ ROC-AUC: > 0.90

   INTERPRETACI√ìN ESPERADA:
   Si Precision=85%, significa:
   "De cada 100 pr√©stamos aprobados,
    85 fueron realmente buenos pagadores"

   Si Recall=80%, significa:
   "De cada 100 buenos pagadores,
    detectamos 80"
```

---

## EJERCICIO 12: Pregunta Te√≥rica Compleja

### Problema
"En un √°rbol de decisi√≥n, ¬øpor qu√© es importante limitar max_depth?
Explica matem√°ticamente qu√© pasa sin l√≠mite y con l√≠mite=5"

### Soluci√≥n
```
SIN L√çMITE DE PROFUNDIDAD (max_depth=None):

El √°rbol crece hasta que todos los nodos hoja sean puros (una sola clase)

Ejemplo visual:
                Root (Gini=0.48)
                /              \
         Nodo_2 (G=0.4)    Nodo_3 (G=0.45)
          /          \       /          \
    Nodo_4 (G=0.35) Hoja_A Hoja_B    Nodo_5 (G=0.4)
     /     \                          /      \
Hoja_C  Hoja_D                   Hoja_E  Hoja_F

... contin√∫a hasta Gini = 0 en todas las hojas

PROBLEMA: Memoriza datos de training
Training Accuracy = 100%
Test Accuracy = 75% ‚ùå Overfitting

RAZ√ìN MATEM√ÅTICA:
P(Overfitting) = (Complejidad - Relevancia)¬≤

Sin l√≠mite:
‚îú‚îÄ Complejidad muy alta (muchas reglas espec√≠ficas)
‚îú‚îÄ Se ajusta a ruido y anomal√≠as
‚îú‚îÄ No generaliza a datos nuevos
‚îî‚îÄ Error en test >>> Error en train


CON L√çMITE max_depth=5:

El √°rbol crece m√°ximo 5 niveles, aunque haya nodos impuros

Ejemplo visual:
                Root (Gini=0.48)
                /              \
         Nodo_2 (G=0.4)    Nodo_3 (G=0.45)
          /          \       /          \
    Nodo_4 (G=0.35) Nodo_5 (G=0.42) Nodo_6 Nodo_7
     /     \        /     \
Hoja_A Hoja_B  Hoja_C  Hoja_D

[Profundidad = 5, STOP aqu√≠ aunque no sean puros]

VENTAJA: Generaliza mejor
Training Accuracy = 87%
Test Accuracy = 85% ‚úÖ Buen balance

RAZ√ìN MATEM√ÅTICA:
Error_Total = Error_Bias + Error_Varianza

Sin l√≠mite:
‚îú‚îÄ Bias bajo (se ajusta bien)
‚îî‚îÄ Varianza ALTA (cambia mucho con datos nuevos)

Con l√≠mite=5:
‚îú‚îÄ Bias un poco m√°s alto (menos flexible)
‚îî‚îÄ Varianza BAJA (m√°s estable con datos nuevos)

CONCLUSI√ìN:
max_depth act√∫a como regularizador

Formula aproximada:
Error_Test ‚âà Error_Train + Œª √ó (Complejidad)

where Œª es mayor cuando max_depth es m√°s bajo
```

---

## CONCLUSI√ìN

**Estos ejercicios cubren:**
- ‚úÖ C√°lculos matem√°ticos (Gini, Bayes)
- ‚úÖ Conceptos pr√°cticos (One-Hot, CV, GridSearch)
- ‚úÖ Interpretaci√≥n de resultados
- ‚úÖ Elecci√≥n de algoritmos
- ‚úÖ Detecci√≥n de problemas

**Para preparar tu examen:**
1. Resuelve TODOS estos ejercicios
2. Entiende cada paso
3. Cubre tus soluciones y rehazlas
4. Varia los n√∫meros y practica m√°s

¬°Mucho √©xito! üéâ

---

*Documento: Ejercicios Pr√°cticos con Soluciones*  
*Miner√≠a de Datos - 8vo Ciclo*
